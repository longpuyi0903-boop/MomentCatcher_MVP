"""
Query Parser - LLM æŸ¥è¯¢ç†è§£
ç”¨ LLM æ›¿ä»£ç¡¬ç¼–ç è§„åˆ™ï¼Œæ™ºèƒ½è§£æç”¨æˆ·æŸ¥è¯¢æ„å›¾

åŠŸèƒ½ï¼š
1. æå–æŸ¥è¯¢å…³é”®è¯
2. è¯†åˆ«å®ä½“ç±»å‹
3. è§£ææ—¶é—´èŒƒå›´
4. åˆ¤æ–­æŸ¥è¯¢ç±»å‹ï¼ˆäº‹å®/æƒ…æ„Ÿ/æ¨¡ç³Šï¼‰
"""

import os
import json
import re
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta

# LLM å®¢æˆ·ç«¯
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


class QueryParser:
    """
    æŸ¥è¯¢ç†è§£å™¨
    
    ä½¿ç”¨ LLM æ™ºèƒ½è§£æç”¨æˆ·æŸ¥è¯¢ï¼Œæå–ï¼š
    - keywords: æ£€ç´¢å…³é”®è¯
    - entity_types: ç›¸å…³å®ä½“ç±»å‹
    - time_range: æ—¶é—´èŒƒå›´
    - query_type: æŸ¥è¯¢ç±»å‹ï¼ˆfact/emotion/fuzzyï¼‰
    - search_strategy: æ¨èæ£€ç´¢ç­–ç•¥
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æŸ¥è¯¢ç†è§£å™¨"""
        self._init_client()
        
        # ç®€å•ç¼“å­˜ï¼ˆé¿å…é‡å¤è°ƒç”¨ï¼‰
        self._cache: Dict[str, Dict] = {}
        self._cache_max_size = 100
    
    def _init_client(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            self.client = None
            return
        
        api_key = os.getenv("ALIYUN_QWEN_KEY")
        if not api_key:
            try:
                from config.api_config import APIConfig
                api_key = APIConfig.QWEN_API_KEY
            except:
                pass
        
        if api_key:
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
        else:
            self.client = None
    
    def parse(self, query: str) -> Dict:
        """
        è§£æç”¨æˆ·æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            Dict: è§£æç»“æœ
            {
                "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
                "entity_types": ["objects", "places", "people", "events"],
                "time_range": {"start": "2024-01-01", "end": "2024-01-07"} or None,
                "query_type": "fact" | "emotion" | "fuzzy",
                "search_strategy": "structured" | "vector" | "hybrid",
                "expanded_queries": ["æ‰©å±•æŸ¥è¯¢1", "æ‰©å±•æŸ¥è¯¢2"],
                "confidence": 0.9
            }
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = query.strip().lower()
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # LLM è§£æ
        if self.client:
            result = self._parse_with_llm(query)
        else:
            # é™çº§åˆ°è§„åˆ™è§£æ
            result = self._parse_with_rules(query)
        
        # ç¼“å­˜ç»“æœ
        if len(self._cache) >= self._cache_max_size:
            # ç®€å•æ¸…ç†ï¼šåˆ é™¤ä¸€åŠ
            keys_to_delete = list(self._cache.keys())[:self._cache_max_size // 2]
            for k in keys_to_delete:
                del self._cache[k]
        
        self._cache[cache_key] = result
        return result
    
    def _parse_with_llm(self, query: str) -> Dict:
        """ä½¿ç”¨ LLM è§£ææŸ¥è¯¢"""
        prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ï¼Œæå–æ£€ç´¢æ‰€éœ€ä¿¡æ¯ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š"{query}"

è¯·è¿”å› JSON æ ¼å¼ï¼ˆä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],  // ç”¨äºæ£€ç´¢çš„æ ¸å¿ƒå…³é”®è¯ï¼ŒåŒ…æ‹¬åŒä¹‰è¯æ‰©å±•
    "entity_types": ["objects"],  // ç›¸å…³å®ä½“ç±»å‹ï¼šobjects/places/people/events/habits
    "time_reference": "today/yesterday/last_week/specific_date/none",  // æ—¶é—´å¼•ç”¨
    "query_type": "fact",  // fact=é—®å…·ä½“äº‹å®, emotion=é—®æ„Ÿå—å›å¿†, fuzzy=æ¨¡ç³ŠæŸ¥è¯¢
    "search_strategy": "hybrid",  // structured=ç²¾ç¡®åŒ¹é…, vector=è¯­ä¹‰åŒ¹é…, hybrid=æ··åˆ
    "expanded_queries": ["æ‰©å±•æŸ¥è¯¢1"],  // è¯­ä¹‰æ‰©å±•çš„æŸ¥è¯¢ï¼ˆç”¨äºå‘é‡æ£€ç´¢ï¼‰
    "confidence": 0.9  // è§£æç½®ä¿¡åº¦
}}

ç¤ºä¾‹1ï¼š
æŸ¥è¯¢ï¼š"ä½ è®°å¾—æˆ‘æ˜¨å¤©å–çš„å’–å•¡æ˜¯ä»€ä¹ˆå£å‘³å—"
è¿”å›ï¼š{{"keywords": ["å’–å•¡", "å£å‘³", "é¥®å“"], "entity_types": ["objects"], "time_reference": "yesterday", "query_type": "fact", "search_strategy": "structured", "expanded_queries": ["æ˜¨å¤©çš„å’–å•¡", "å–çš„é¥®æ–™"], "confidence": 0.95}}

ç¤ºä¾‹2ï¼š
æŸ¥è¯¢ï¼š"ä¸Šæ¬¡æˆ‘å¿ƒæƒ…ä¸å¥½çš„æ—¶å€™"
è¿”å›ï¼š{{"keywords": ["å¿ƒæƒ…ä¸å¥½", "éš¾è¿‡", "ä¼¤å¿ƒ"], "entity_types": ["events"], "time_reference": "none", "query_type": "emotion", "search_strategy": "vector", "expanded_queries": ["æƒ…ç»ªä½è½", "ä¸å¼€å¿ƒçš„äº‹"], "confidence": 0.8}}

ç¤ºä¾‹3ï¼š
æŸ¥è¯¢ï¼š"ä¹‹å‰èŠè¿‡çš„é‚£ä¸ªäº‹"
è¿”å›ï¼š{{"keywords": [], "entity_types": [], "time_reference": "none", "query_type": "fuzzy", "search_strategy": "vector", "expanded_queries": ["ä¹‹å‰çš„å¯¹è¯", "èŠè¿‡çš„è¯é¢˜"], "confidence": 0.5}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model="qwen-turbo",  # ç”¨å¿«é€Ÿæ¨¡å‹ï¼ŒèŠ‚çœæˆæœ¬
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=500
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # æ¸…ç† markdown æ ‡è®°
            if result_text.startswith("```json"):
                result_text = result_text[7:]
            if result_text.startswith("```"):
                result_text = result_text[3:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
            result_text = result_text.strip()
            
            # è§£æ JSON
            result = json.loads(result_text)
            
            # å¤„ç†æ—¶é—´å¼•ç”¨
            result["time_range"] = self._parse_time_reference(
                result.get("time_reference", "none")
            )
            
            return result
            
        except Exception as e:
            print(f"   âš ï¸ LLM è§£æå¤±è´¥: {e}ï¼Œé™çº§åˆ°è§„åˆ™è§£æ")
            return self._parse_with_rules(query)
    
    def _parse_with_rules(self, query: str) -> Dict:
        """è§„åˆ™è§£æï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        keywords = []
        entity_types = []
        query_type = "fuzzy"
        search_strategy = "hybrid"
        
        # ç®€å•å…³é”®è¯æå–
        # ç§»é™¤åœç”¨è¯ååˆ†è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'å—', 'å‘¢', 'å•Š', 'å§', 
                     'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'è®°å¾—', 'è®°ä¸è®°å¾—', 'è¿˜', 'æœ‰', 'æ²¡æœ‰'}
        
        words = list(query)
        # ç®€å•çš„ 2-gram
        for i in range(len(words) - 1):
            bigram = words[i] + words[i+1]
            if bigram not in stopwords and len(bigram) == 2:
                keywords.append(bigram)
        
        # æ£€æµ‹å®ä½“ç±»å‹
        if re.search(r'(å’–å•¡|æ‹¿é“|å¥¶èŒ¶|åƒ|å–|é¥­|èœ)', query):
            entity_types.append("objects")
            keywords.extend(re.findall(r'(å’–å•¡|æ‹¿é“|å¥¶èŒ¶|é¥­|èœ|èŒ¶)', query))
        
        if re.search(r'(å…¬å¸|å­¦æ ¡|å®¶|åº—|å“ªé‡Œ|åœ¨å“ª)', query):
            entity_types.append("places")
        
        if re.search(r'(è°|æœ‹å‹|åŒäº‹|å®¶äºº)', query):
            entity_types.append("people")
        
        # æ£€æµ‹æŸ¥è¯¢ç±»å‹
        if re.search(r'(ä»€ä¹ˆ|å“ªä¸ª|å‡ |å¤šå°‘|æ˜¯ä¸æ˜¯)', query):
            query_type = "fact"
            search_strategy = "structured"
        elif re.search(r'(å¿ƒæƒ…|æ„Ÿè§‰|å¼€å¿ƒ|éš¾è¿‡|æƒ…ç»ª)', query):
            query_type = "emotion"
            search_strategy = "vector"
        
        # æ£€æµ‹æ—¶é—´
        time_range = None
        if "ä»Šå¤©" in query:
            time_range = self._parse_time_reference("today")
        elif "æ˜¨å¤©" in query:
            time_range = self._parse_time_reference("yesterday")
        elif "ä¸Šå‘¨" in query or "æœ€è¿‘" in query:
            time_range = self._parse_time_reference("last_week")
        
        # å»é‡
        keywords = list(set(keywords))[:10]
        entity_types = list(set(entity_types)) or ["objects", "events"]
        
        return {
            "keywords": keywords,
            "entity_types": entity_types,
            "time_reference": "none",
            "time_range": time_range,
            "query_type": query_type,
            "search_strategy": search_strategy,
            "expanded_queries": [query],
            "confidence": 0.5
        }
    
    def _parse_time_reference(self, ref: str) -> Optional[Dict]:
        """è§£ææ—¶é—´å¼•ç”¨ä¸ºå…·ä½“æ—¥æœŸèŒƒå›´"""
        now = datetime.now()
        
        if ref == "today":
            start = now.replace(hour=0, minute=0, second=0, microsecond=0)
            return {
                "start": start.isoformat(),
                "end": now.isoformat()
            }
        
        elif ref == "yesterday":
            yesterday = now - timedelta(days=1)
            start = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
            end = yesterday.replace(hour=23, minute=59, second=59)
            return {
                "start": start.isoformat(),
                "end": end.isoformat()
            }
        
        elif ref == "last_week":
            start = now - timedelta(days=7)
            return {
                "start": start.isoformat(),
                "end": now.isoformat()
            }
        
        elif ref == "last_month":
            start = now - timedelta(days=30)
            return {
                "start": start.isoformat(),
                "end": now.isoformat()
            }
        
        return None
    
    def get_search_config(self, query: str) -> Dict:
        """
        è·å–æ£€ç´¢é…ç½®ï¼ˆç®€åŒ–æ¥å£ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            Dict: æ£€ç´¢é…ç½®
            {
                "use_structured": True,
                "use_vector": True,
                "structured_weight": 0.6,
                "vector_weight": 0.4,
                "keywords": [...],
                "entity_types": [...],
                "time_range": {...}
            }
        """
        parsed = self.parse(query)
        
        strategy = parsed.get("search_strategy", "hybrid")
        
        if strategy == "structured":
            return {
                "use_structured": True,
                "use_vector": False,
                "structured_weight": 1.0,
                "vector_weight": 0.0,
                "keywords": parsed.get("keywords", []),
                "entity_types": parsed.get("entity_types", []),
                "time_range": parsed.get("time_range"),
                "expanded_queries": parsed.get("expanded_queries", [query])
            }
        
        elif strategy == "vector":
            return {
                "use_structured": False,
                "use_vector": True,
                "structured_weight": 0.0,
                "vector_weight": 1.0,
                "keywords": parsed.get("keywords", []),
                "entity_types": parsed.get("entity_types", []),
                "time_range": parsed.get("time_range"),
                "expanded_queries": parsed.get("expanded_queries", [query])
            }
        
        else:  # hybrid
            # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´æƒé‡
            confidence = parsed.get("confidence", 0.5)
            if confidence > 0.8:
                # é«˜ç½®ä¿¡åº¦ï¼šç»“æ„åŒ–ä¼˜å…ˆ
                structured_weight = 0.7
            elif confidence > 0.5:
                # ä¸­ç½®ä¿¡åº¦ï¼šå‡è¡¡
                structured_weight = 0.5
            else:
                # ä½ç½®ä¿¡åº¦ï¼šå‘é‡ä¼˜å…ˆ
                structured_weight = 0.3
            
            return {
                "use_structured": True,
                "use_vector": True,
                "structured_weight": structured_weight,
                "vector_weight": 1 - structured_weight,
                "keywords": parsed.get("keywords", []),
                "entity_types": parsed.get("entity_types", []),
                "time_range": parsed.get("time_range"),
                "expanded_queries": parsed.get("expanded_queries", [query])
            }


# å…¨å±€å•ä¾‹ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
_query_parser: Optional[QueryParser] = None


def get_query_parser() -> QueryParser:
    """è·å–æŸ¥è¯¢è§£æå™¨å•ä¾‹"""
    global _query_parser
    if _query_parser is None:
        _query_parser = QueryParser()
    return _query_parser


# ============================================================
# æµ‹è¯•ä»£ç 
# ============================================================

def test_query_parser():
    """æµ‹è¯•æŸ¥è¯¢è§£æå™¨"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯• QueryParser")
    print("="*60 + "\n")
    
    parser = QueryParser()
    
    test_queries = [
        "ä½ è®°å¾—æˆ‘æ˜¨å¤©å–çš„å’–å•¡æ˜¯ä»€ä¹ˆå£å‘³å—",
        "ä¸Šæ¬¡æˆ‘å¿ƒæƒ…ä¸å¥½æ˜¯å› ä¸ºä»€ä¹ˆ",
        "ä¹‹å‰èŠè¿‡çš„é‚£ä¸ªäº‹",
        "æˆ‘æ–¹æ¡ˆçš„é…è‰²æ˜¯ä»€ä¹ˆ",
        "åœ¨æ˜Ÿå·´å…‹ä¹°çš„é‚£æ¯",
        "æœ€è¿‘å·¥ä½œä¸Šæœ‰ä»€ä¹ˆå¼€å¿ƒçš„äº‹å—"
    ]
    
    for query in test_queries:
        print(f"ğŸ” æŸ¥è¯¢: '{query}'")
        result = parser.parse(query)
        print(f"   å…³é”®è¯: {result.get('keywords', [])}")
        print(f"   å®ä½“ç±»å‹: {result.get('entity_types', [])}")
        print(f"   æŸ¥è¯¢ç±»å‹: {result.get('query_type', '')}")
        print(f"   æ£€ç´¢ç­–ç•¥: {result.get('search_strategy', '')}")
        print(f"   æ‰©å±•æŸ¥è¯¢: {result.get('expanded_queries', [])}")
        print(f"   æ—¶é—´èŒƒå›´: {result.get('time_range', None)}")
        print(f"   ç½®ä¿¡åº¦: {result.get('confidence', 0)}")
        print()
    
    print("="*60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*60 + "\n")


if __name__ == "__main__":
    test_query_parser()
