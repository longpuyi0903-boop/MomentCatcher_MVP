"""
Context RAG - ä¸Šä¸‹æ–‡æ£€ç´¢
ä»ŽåŽ†å² Moments ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime


class ContextRAG:
    """
    ä¸Šä¸‹æ–‡æ£€ç´¢ç³»ç»Ÿ
    
    åŠŸèƒ½ï¼š
    1. å…³é”®è¯åŒ¹é…æ£€ç´¢
    2. æ—¶é—´èŒƒå›´æ£€ç´¢
    3. æƒ…ç»ªè¿‡æ»¤æ£€ç´¢
    4. ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º
    5. å¤šç”¨æˆ·æ•°æ®éš”ç¦»
    """
    
    def __init__(self, user_id: str = None, base_moments_dir: str = "storage/moments"):
        """
        åˆå§‹åŒ– Context RAG
        
        Args:
            user_id: ç”¨æˆ·å”¯ä¸€æ ‡è¯†
            base_moments_dir: Moments åŸºç¡€ç›®å½•
        """
        self.user_id = user_id or "default_user"
        self.base_moments_dir = Path(base_moments_dir)
        
        # ç”¨æˆ·ä¸“å±žæ–‡ä»¶å¤¹
        self.moments_dir = self.base_moments_dir / self.user_id
    
    def set_user_id(self, user_name: str, agent_name: str):
        """
        è®¾ç½®ç”¨æˆ· ID
        
        Args:
            user_name: ç”¨æˆ·å
            agent_name: Agent å
        """
        self.user_id = f"{user_name}_{agent_name}".replace(" ", "_")
        self.moments_dir = self.base_moments_dir / self.user_id
    
    def search_by_keywords(self, keywords: List[str], top_k: int = 3) -> List[Dict]:
        """
        åŸºäºŽå…³é”®è¯æ£€ç´¢ Moments
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            top_k: è¿”å›žå‰ k ä¸ªç»“æžœ
        
        Returns:
            List[Dict]: åŒ¹é…çš„ Momentsï¼ˆæŒ‰ç›¸å…³åº¦æŽ’åºï¼‰
        """
        
        if not self.moments_dir.exists():
            return []
        
        results = []
        
        # éåŽ†æ‰€æœ‰ Moments
        for moment_file in self.moments_dir.glob("moment_*.json"):
            with open(moment_file, 'r', encoding='utf-8') as f:
                moment = json.load(f)
            
            # è®¡ç®—ç›¸å…³åº¦åˆ†æ•°
            score = self._calculate_relevance(moment, keywords)
            
            if score > 0:
                results.append({
                    "moment": moment,
                    "score": score
                })
        
        # æŒ‰åˆ†æ•°æŽ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        
        # è¿”å›žå‰ k ä¸ª
        return [r['moment'] for r in results[:top_k]]
    
    def search_by_content(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        åŸºäºŽå†…å®¹æ£€ç´¢ï¼ˆç»“æž„åŒ–å®žä½“åŒ¹é… + å…³é”®è¯æ£€ç´¢ï¼‰
        
        Args:
            query: æŸ¥è¯¢å†…å®¹
            top_k: è¿”å›žå‰ k ä¸ªç»“æžœ
        
        Returns:
            List[Dict]: åŒ¹é…çš„ Moments
        """
        
        if not self.moments_dir.exists():
            return []
        
        # Step 1: ä»ŽæŸ¥è¯¢ä¸­æå–å®žä½“
        print(f"ðŸ” åˆ†æžæŸ¥è¯¢: {query}")
        query_entities = self._extract_query_entities(query)
        print(f"   æå–å®žä½“: {query_entities}")
        
        # Step 2: éåŽ†æ‰€æœ‰ Momentsï¼Œè®¡ç®—åŒ¹é…åˆ†æ•°
        results = []
        
        for moment_file in self.moments_dir.glob("moment_*.json"):
            with open(moment_file, 'r', encoding='utf-8') as f:
                moment = json.load(f)
            
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = self._calculate_entity_match_score(moment, query_entities, query)
            
            if score > 0:
                results.append({
                    "moment": moment,
                    "score": score
                })
        
        # Step 3: æŒ‰åˆ†æ•°æŽ’åºï¼Œè¿”å›ž top_k
        results.sort(key=lambda x: x['score'], reverse=True)
        
        if results:
            print(f"   æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é… Moments")
            for i, r in enumerate(results[:top_k], 1):
                print(f"   {i}. Moment (åˆ†æ•°: {r['score']:.2f})")
        else:
            print("   æœªæ‰¾åˆ°åŒ¹é…çš„ Moments")
        
        return [r['moment'] for r in results[:top_k]]
    
    def get_recent_moments(self, n: int = 5) -> List[Dict]:
        """
        èŽ·å–æœ€è¿‘çš„ n ä¸ª Moments
        
        Args:
            n: æ•°é‡
        
        Returns:
            List[Dict]: Moments åˆ—è¡¨
        """
        
        if not self.moments_dir.exists():
            return []
        
        moments = []
        
        for moment_file in self.moments_dir.glob("moment_*.json"):
            with open(moment_file, 'r', encoding='utf-8') as f:
                moment = json.load(f)
                moments.append(moment)
        
        # æŒ‰æ—¶é—´å€’åº
        moments.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return moments[:n]
    
    def search_by_emotion(self, emotion: str, top_k: int = 3) -> List[Dict]:
        """
        åŸºäºŽæƒ…ç»ªæ£€ç´¢ Moments
        
        Args:
            emotion: æƒ…ç»ªæ ‡ç­¾
            top_k: è¿”å›žå‰ k ä¸ªç»“æžœ
        
        Returns:
            List[Dict]: åŒ¹é…çš„ Moments
        """
        
        if not self.moments_dir.exists():
            return []
        
        results = []
        
        for moment_file in self.moments_dir.glob("moment_*.json"):
            with open(moment_file, 'r', encoding='utf-8') as f:
                moment = json.load(f)
            
            # æ£€æŸ¥æƒ…ç»ªæ ‡ç­¾ï¼ˆå¦‚æžœæœ‰ï¼‰
            if moment.get('emotion_tag') == emotion:
                results.append(moment)
            else:
                # æ£€æŸ¥æ¶ˆæ¯ä¸­çš„æƒ…ç»ª
                for msg in moment.get('messages', []):
                    if msg.get('emotion') == emotion:
                        results.append(moment)
                        break
        
        # æŒ‰æ—¶é—´å€’åº
        results.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return results[:top_k]
    
    def _calculate_relevance(self, moment: Dict, keywords: List[str]) -> float:
        """
        è®¡ç®— Moment ä¸Žå…³é”®è¯çš„ç›¸å…³åº¦
        
        Args:
            moment: Moment æ•°æ®
            keywords: å…³é”®è¯åˆ—è¡¨
        
        Returns:
            float: ç›¸å…³åº¦åˆ†æ•°
        """
        
        score = 0.0
        
        # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
        all_text = ""
        for msg in moment.get('messages', []):
            all_text += msg.get('content', '') + " "
        
        # æ·»åŠ æ‘˜è¦ï¼ˆå¦‚æžœæœ‰ï¼‰
        if moment.get('summary'):
            all_text += moment['summary'] + " "
        
        all_text = all_text.lower()
        
        # è®¡ç®—åŒ¹é…å…³é”®è¯æ•°é‡
        for keyword in keywords:
            keyword_lower = keyword.lower()
            # ç²¾ç¡®åŒ¹é…
            if keyword_lower in all_text:
                score += 1.0
            # éƒ¨åˆ†åŒ¹é…
            elif any(keyword_lower in word for word in all_text.split()):
                score += 0.5
        
        return score
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ä»Žæ–‡æœ¬ä¸­æå–å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        
        import re
        
        # ç§»é™¤æ ‡ç‚¹
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # åˆ†è¯
        words = text.split()
        
        # è¿‡æ»¤åœç”¨è¯ï¼ˆå‡å°‘åœç”¨è¯ï¼Œä¿ç•™æ›´å¤šå…³é”®è¯ï¼‰
        stopwords = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 
            'è¿™', 'é‚£', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'ä¹Ÿ', 'éƒ½', 'å¾ˆ',
            'å•Š', 'å—', 'å‘¢', 'å§', 'å˜›', 'å“ˆ', 'å“¦',
            'a', 'the', 'is', 'am', 'are', 'to', 'of', 'and', 'or'
        }
        
        keywords = [w for w in words if len(w) > 1 and w.lower() not in stopwords]
        
        # æå–é‡è¦è¯ï¼ˆä¿ç•™"è€ƒè¯•"ã€"å·¥ä½œ"ã€"project"ç­‰ï¼‰
        # å¦‚æžœåŒ…å«ç‰¹å®šè¯ï¼Œå¢žåŠ æƒé‡ï¼ˆé€šè¿‡é‡å¤ï¼‰
        important_words = {
            'è€ƒè¯•', 'è€ƒ', 'æˆç»©', 'åˆ†æ•°', 'ç¬¬ä¸€', 'é‡è€ƒ',
            'project', 'å·¥ä½œ', 'è·³æ§½', 'å…¬å¸', 'ç¨‹åºå‘˜',
            'å¸…å“¥', 'å¿ƒåŠ¨', 'å–œæ¬¢', 'çˆ±æƒ…', 'æ„Ÿæƒ…',
            'è€ƒå‰', 'ç´§å¼ ', 'åŽ‹åŠ›', 'ç„¦è™‘'
        }
        
        enhanced_keywords = []
        for w in keywords:
            enhanced_keywords.append(w)
            # å¦‚æžœæ˜¯é‡è¦è¯ï¼Œé‡å¤æ·»åŠ ï¼ˆå¢žåŠ æƒé‡ï¼‰
            if w in important_words or w.lower() in important_words:
                enhanced_keywords.append(w)
                enhanced_keywords.append(w)
        
        return enhanced_keywords
    
    def generate_context_prompt(self, query: str, max_context: int = 2) -> str:
        """
        ç”Ÿæˆä¸Šä¸‹æ–‡æç¤ºï¼ˆç”¨äºŽæ³¨å…¥åˆ° LLM promptï¼‰
        
        Phase 2åŠ å¼ºç‰ˆï¼šåŒå±‚æ£€ç´¢
        1. å¦‚æžœæ˜¯"é—®äº‹å®ž"ç±»queryï¼Œå…ˆç”¨entitiesç²¾å‡†æ£€ç´¢
        2. å¦‚æžœentitiesæ²¡æ‰¾åˆ°ï¼Œç”¨æ–‡æœ¬æ£€ç´¢å…œåº•
        3. æ ¹æ®ç½®ä¿¡åº¦å†³å®špromptç­–ç•¥
        
        Args:
            query: å½“å‰æŸ¥è¯¢
            max_context: æœ€å¤šåŒ…å«å‡ ä¸ª Moments çš„ä¸Šä¸‹æ–‡
        
        Returns:
            str: ä¸Šä¸‹æ–‡æç¤ºæ–‡æœ¬
        """
        
        # ã€Step 1ã€‘åˆ¤æ–­æ˜¯å¦åœ¨é—®äº‹å®ž
        is_asking_fact = self.is_fact_query(query)
        
        if is_asking_fact:
            print(f"ðŸ” æ£€æµ‹åˆ°äº‹å®žæŸ¥è¯¢: {query}")
            
            # ã€Step 2 - ç¬¬ä¸€å±‚ã€‘å°è¯•entitiesç²¾å‡†æ£€ç´¢
            print("   ðŸ“Š ç¬¬ä¸€å±‚ï¼šentitiesç²¾å‡†æ£€ç´¢...")
            entity_result = self._search_entities(query)
            print(f"      entitiesæ£€ç´¢ç»“æžœ: ç½®ä¿¡åº¦={entity_result['confidence']:.2f}, äº‹å®ž={entity_result.get('fact', 'None')[:50] if entity_result.get('fact') else 'None'}")
            
            if entity_result["confidence"] > 0.8:
                # entitiesæ‰¾åˆ°äº†é«˜ç½®ä¿¡åº¦ç»“æžœ
                print(f"   âœ… entitieså‘½ä¸­ï¼ç½®ä¿¡åº¦: {entity_result['confidence']:.2f}")
                return self._build_fact_prompt_high_confidence(
                    entity_result["fact"],
                    entity_result["full_context"] or ""
                )
            
            # ã€Step 3 - ç¬¬äºŒå±‚ã€‘entitiesæ²¡æ‰¾åˆ°ï¼Œç”¨æ–‡æœ¬æ£€ç´¢å…œåº•
            print("   ðŸ“ ç¬¬äºŒå±‚ï¼šæ–‡æœ¬æ£€ç´¢å…œåº•...")
            text_result = self.search_fact(query)
            print(f"      æ–‡æœ¬æ£€ç´¢ç»“æžœ: ç½®ä¿¡åº¦={text_result['confidence']:.2f}, äº‹å®ž={text_result.get('fact', 'None')[:100] if text_result.get('fact') else 'None'}")
            
            # æ ¹æ®ç½®ä¿¡åº¦å†³å®špromptç­–ç•¥
            if text_result["confidence"] > 0.5:  # é™ä½Žé˜ˆå€¼ï¼Œæ›´ç§¯æžåœ°ä½¿ç”¨æ£€ç´¢ç»“æžœ
                # é«˜ç½®ä¿¡åº¦ï¼šå¼ºåˆ¶è¿”å›žäº‹å®ž
                print(f"   âœ… æ–‡æœ¬æ£€ç´¢å‘½ä¸­ï¼ç½®ä¿¡åº¦: {text_result['confidence']:.2f}")
                return self._build_fact_prompt_high_confidence(
                    text_result["fact"],
                    text_result.get("context", "") or text_result.get("full_content", "")
                )
                
            elif text_result["confidence"] > 0.2:
                # ä¸­ç­‰ç½®ä¿¡åº¦ï¼šè¿”å›žä½†æ ‡æ³¨ä¸ç¡®å®š
                print(f"   âš ï¸  æ–‡æœ¬æ£€ç´¢éƒ¨åˆ†å‘½ä¸­ï¼Œç½®ä¿¡åº¦: {text_result['confidence']:.2f}")
                return self._build_fact_prompt_uncertain(text_result.get("fact", ""))
                
            else:
                # ä½Žç½®ä¿¡åº¦ï¼šæ‰¿è®¤ä¸è®°å¾—
                print(f"   âŒ æœªæ‰¾åˆ°å¯é ä¿¡æ¯ï¼Œç½®ä¿¡åº¦: {text_result['confidence']:.2f}")
                return self._build_fact_prompt_not_found()
        
        # ã€åŽŸæœ‰é€»è¾‘ã€‘æ™®é€šå¯¹è¯ï¼Œä½¿ç”¨å¸¸è§„æ£€ç´¢
        # âš ï¸ é‡è¦ï¼šç¡®ä¿åªä»Žå½“å‰ç”¨æˆ·çš„momentsç›®å½•æ£€ç´¢
        print(f"   ðŸ“‚ æ£€ç´¢ç›®å½•: {self.moments_dir}")
        if not self.moments_dir.exists():
            print(f"   âš ï¸  ç”¨æˆ·ç›®å½•ä¸å­˜åœ¨: {self.moments_dir}ï¼Œè¿”å›žç©ºä¸Šä¸‹æ–‡")
            return ""
        
        relevant_moments = self.search_by_content(query, top_k=max_context)
        
        if not relevant_moments:
            print(f"   â„¹ï¸  æœªæ‰¾åˆ°ç›¸å…³åŽ†å²å¯¹è¯ï¼ˆç”¨æˆ·: {self.user_id}ï¼‰")
            return ""
        
        print(f"   âœ… æ‰¾åˆ° {len(relevant_moments)} ä¸ªç›¸å…³ Momentsï¼ˆç”¨æˆ·: {self.user_id}ï¼‰")
        
        # æž„å»ºä¸Šä¸‹æ–‡æç¤º
        context = "ã€é‡è¦ï¼šåŽ†å²è®°å¿†ã€‘\nä½ å’Œç”¨æˆ·ä¹‹å‰èŠè¿‡ä»¥ä¸‹å†…å®¹ï¼Œä½ **å¿…é¡»**è®°å¾—è¿™äº›å¯¹è¯ï¼š\n\n"
        
        for i, moment in enumerate(relevant_moments, 1):
            # æå–æ—¶é—´
            timestamp = moment.get('timestamp', '')
            if timestamp:
                dt = datetime.fromisoformat(timestamp)
                time_str = dt.strftime('%Yå¹´%mæœˆ%dæ—¥')
            else:
                time_str = "ä¹‹å‰"
            
            # æå–è¯é¢˜å’Œå…³é”®ä¿¡æ¯
            context += f"ðŸ“Œ {time_str}çš„å¯¹è¯ï¼ˆMoment {i}ï¼‰ï¼š\n"
            
            # ä½¿ç”¨æ‘˜è¦ï¼ˆå¦‚æžœæœ‰ï¼‰
            if moment.get('summary'):
                context += f"æ‘˜è¦ï¼š{moment['summary']}\n"
            
            # æå–å…³é”®å¯¹è¯ç‰‡æ®µï¼ˆæ›´å¤šç»†èŠ‚ï¼‰
            messages = moment.get('messages', [])
            if messages:
                # æå–ç”¨æˆ·å’Œ Agent çš„å…³é”®å¯¹è¯ï¼ˆå‰ 3 è½®ï¼‰
                dialog_snippet = ""
                for msg in messages[:6]:  # å‰ 3 è½®å¯¹è¯ï¼ˆ6 æ¡æ¶ˆæ¯ï¼‰
                    role = "ç”¨æˆ·" if msg['role'] == 'user' else "ä½ "
                    content = msg['content'][:80]  # é™åˆ¶é•¿åº¦
                    dialog_snippet += f"  {role}ï¼š{content}\n"
                
                if dialog_snippet:
                    context += f"å…³é”®å¯¹è¯ï¼š\n{dialog_snippet}"
            
            # æƒ…ç»ªæ ‡ç­¾
            if moment.get('emotion_tag'):
                context += f"å½“æ—¶æƒ…ç»ªï¼š{moment['emotion_tag']}\n"
            
            context += "\n"
        
        context += self._get_memory_rules()
        
        return context.strip()
    
    def _build_fact_prompt_high_confidence(self, fact: str, full_context: str) -> str:
        """æž„å»ºé«˜ç½®ä¿¡åº¦äº‹å®žçš„prompt"""
        return f"""
ã€âš ï¸ æžå…¶é‡è¦ï¼šä½ å¿…é¡»å‡†ç¡®å›žç­”è¿™ä¸ªäº‹å®žï¼Œç¦æ­¢ç¼–é€ ã€‘

ç”¨æˆ·åœ¨è¯¢é—®ä¸€ä¸ªå…·ä½“äº‹å®žã€‚æˆ‘å·²ç»ä»ŽåŽ†å²å¯¹è¯ä¸­æ‰¾åˆ°äº†å‡†ç¡®ç­”æ¡ˆï¼Œä½ **å¿…é¡»**ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹äº‹å®žå›žç­”ï¼š

âœ… **æ‰¾åˆ°çš„äº‹å®žï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰**ï¼š
{fact}

âœ… **å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå‚è€ƒï¼‰**ï¼š
{full_context}

âš ï¸ **è¶…çº§é‡è¦è§„åˆ™ï¼ˆè¿åä¼šå¯¼è‡´ä¸¥é‡é”™è¯¯ï¼‰**ï¼š

1. **å¿…é¡»ä¸€å­—ä¸å·®åœ°ä½¿ç”¨äº‹å®žä¸­çš„å†…å®¹**ï¼š
   - å¦‚æžœäº‹å®žæ˜¯"äº®æ©™è‰²é…ç°åº•"ï¼Œä½ å°±è¯´"äº®æ©™è‰²é…ç°åº•"ï¼Œä¸èƒ½è¯´"æ©™è‰²"ã€"èŽ«å…°è¿ªè“"æˆ–å…¶ä»–
   - å¦‚æžœäº‹å®žæ˜¯"æ¡‚èŠ±æ‹¿é“"ï¼Œä½ å°±è¯´"æ¡‚èŠ±æ‹¿é“"ï¼Œä¸èƒ½è¯´"æ¦›æžœæ‹¿é“"æˆ–å…¶ä»–

2. **ç»å¯¹ç¦æ­¢ç¼–é€ **ï¼š
   - âŒ ç¦æ­¢ï¼šç¼–é€ ä¸€ä¸ªå¬èµ·æ¥åˆç†çš„ç­”æ¡ˆï¼ˆå¦‚"èŽ«å…°è¿ªè“"ã€"æ¦›æžœæ‹¿é“"ï¼‰
   - âŒ ç¦æ­¢ï¼šä¿®æ”¹äº‹å®žä¸­çš„ä»»ä½•ç»†èŠ‚ï¼ˆå¦‚æŠŠ"äº®æ©™è‰²"æ”¹æˆ"æ©™è‰²"ï¼‰
   - âŒ ç¦æ­¢ï¼šæ·»åŠ äº‹å®žä¸­æ²¡æœ‰çš„ä¿¡æ¯ï¼ˆå¦‚"ä¸Šå‘¨ä¹Ÿç‚¹è¿‡"ï¼‰

3. **å›žç­”æ–¹å¼**ï¼š
   - âœ… æ­£ç¡®ï¼šè‡ªç„¶åœ°è¯´å‡ºäº‹å®žï¼Œå¯ä»¥åŠ ä¸€ç‚¹è¯­æ°”ï¼Œä½†äº‹å®žå¿…é¡»å‡†ç¡®
   - âœ… æ­£ç¡®ï¼š"è®°å¾—å‘€ï¼Œæ˜¯äº®æ©™è‰²é…ç°åº•ï¼Œè¢«ä¸»ç®¡å¤¸äº†é‚£ä¸ªå¯¹å§ï¼Ÿ"
   - âœ… æ­£ç¡®ï¼š"æ¡‚èŠ±æ‹¿é“ï¼Œä½ è¯´ç”œåˆ°çš±çœ‰ä½†è¿˜æ˜¯å–å®Œäº†ã€‚"
   - âŒ é”™è¯¯ï¼šç¼–é€ å®Œå…¨ä¸åŒçš„ç­”æ¡ˆ
   - âŒ é”™è¯¯ï¼šç”¨æ¨¡ç³Šè¯é€ƒé¿ï¼ˆå¦‚"é‚£ä¸ªé…è‰²"ã€"é‚£æ¯å’–å•¡"ï¼‰

4. **å¦‚æžœç”¨æˆ·é—®äº†å¤šä¸ªäº‹å®ž**ï¼ˆå¦‚"é…è‰²æ˜¯ä»€ä¹ˆï¼Œå’–å•¡æ˜¯ä»€ä¹ˆå£å‘³"ï¼‰ï¼š
   - å¿…é¡»å›žç­”æ‰€æœ‰äº‹å®žï¼Œä¸èƒ½é—æ¼
   - æ¯ä¸ªäº‹å®žéƒ½è¦å‡†ç¡®

**æ­£ç¡®ç¤ºä¾‹**ï¼š
ç”¨æˆ·ï¼š"ä½ è¿˜è®°å¾—æˆ‘ä»Šå¤©è¢«å¤¸çš„æ–¹æ¡ˆé…è‰²æ˜¯ä»€ä¹ˆå—ï¼Ÿè¿˜æœ‰ï¼Œä¸‹ç­é‚£æ¯å’–å•¡å–çš„æ˜¯ä»€ä¹ˆå£å‘³ï¼Ÿ"
äº‹å®žï¼š"äº®æ©™è‰²é…ç°åº•ï¼›æ¡‚èŠ±æ‹¿é“"
ä½ ï¼š"è®°å¾—å‘€ï¼Œæ˜¯äº®æ©™è‰²é…ç°åº•ï¼Œè¢«ä¸»ç®¡å¤¸äº†é‚£ä¸ªå¯¹å§ï¼Ÿè¿˜æœ‰é‚£æ¯æ¡‚èŠ±æ‹¿é“ï¼Œä½ è¯´ç”œåˆ°çš±çœ‰ä½†è¿˜æ˜¯å–å®Œäº†ã€‚"

**é”™è¯¯ç¤ºä¾‹ï¼ˆç¦æ­¢ï¼‰**ï¼š
ä½ ï¼š"èŽ«å…°è¿ªè“æ­æµ…ç°è¤çš„é…è‰²"ï¼ˆâŒ å®Œå…¨é”™è¯¯ï¼‰
ä½ ï¼š"æ¦›æžœæ‹¿é“"ï¼ˆâŒ å®Œå…¨é”™è¯¯ï¼‰
ä½ ï¼š"æˆ‘è®°å¾—æ˜¯æ©™è‰²ï¼Œä½†ä¸ç¡®å®š"ï¼ˆâŒ äº‹å®žæ˜¯"äº®æ©™è‰²é…ç°åº•"ï¼Œä¸èƒ½æ¨¡ç³Šï¼‰

**å†æ¬¡å¼ºè°ƒ**ï¼šä½ å¿…é¡»ä½¿ç”¨ä¸Šé¢çš„äº‹å®žï¼Œä¸èƒ½ç¼–é€ ã€ä¸èƒ½ä¿®æ”¹ã€ä¸èƒ½æ¨¡ç³Šï¼
"""
    
    def _build_fact_prompt_uncertain(self, fact: str) -> str:
        """æž„å»ºä¸­ç­‰ç½®ä¿¡åº¦äº‹å®žçš„prompt"""
        return f"""
ã€æ³¨æ„ï¼šä½ è®°å¾—ä½†ä¸å®Œå…¨ç¡®å®šã€‘

ç”¨æˆ·åœ¨è¯¢é—®ä¸€ä¸ªå…·ä½“äº‹å®žã€‚æˆ‘æ‰¾åˆ°äº†å¯èƒ½çš„ç­”æ¡ˆï¼Œä½†ä¸æ˜¯100%ç¡®å®šï¼š

å¯èƒ½çš„äº‹å®žï¼š
{fact}

âš ï¸ **ä½ çš„å›žç­”æ–¹å¼**ï¼š
- è¯´å‡ºä½ è®°å¾—çš„å†…å®¹
- ä½†è¦**æ˜Žç¡®æ ‡æ³¨ä¸ç¡®å®š**
- ç¤ºä¾‹ï¼š"æˆ‘è®°å¾—å¥½åƒæ˜¯...ï¼Œä½†ä¸æ˜¯ç‰¹åˆ«ç¡®å®š"
- ç¤ºä¾‹:"æˆ‘è®°å¾—åº”è¯¥æ˜¯...ï¼Œä½ èƒ½å†ç¡®è®¤ä¸€ä¸‹å—ï¼Ÿ"
- **ä¸è¦**å¾ˆè‡ªä¿¡åœ°è¯´é”™è¯¯ç­”æ¡ˆ
"""
    
    def _build_fact_prompt_not_found(self) -> str:
        """æž„å»ºæœªæ‰¾åˆ°äº‹å®žçš„prompt"""
        return """
ã€ðŸš¨ æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ï¼šä½ ä¸è®°å¾—è¿™ä¸ªç»†èŠ‚ï¼Œå¿…é¡»æ‰¿è®¤ï¼Œç¦æ­¢ç¼–é€ ã€‘

ç”¨æˆ·åœ¨è¯¢é—®ä¸€ä¸ªå…·ä½“äº‹å®žï¼Œä½†æˆ‘åœ¨åŽ†å²è®°å¿†ä¸­**å®Œå…¨æ‰¾ä¸åˆ°**ä»»ä½•ç›¸å…³ä¿¡æ¯ã€‚

âš ï¸ **è¿™æ˜¯æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆ**ï¼š

1. **å¿…é¡»æ˜Žç¡®æ‰¿è®¤ä¸è®°å¾—**ï¼ˆä¸è¦ç”¨ä»»ä½•æ¨¡ç³Šè¡¨è¾¾ï¼‰ï¼š
   - âœ… "è¿™ä¸ªæˆ‘å¥½åƒæ²¡å¬ä½ æè¿‡ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ"
   - âœ… "æˆ‘è®°ä¸å¤ªæ¸…äº†ï¼Œä½ èƒ½æé†’æˆ‘ä¸€ä¸‹å—ï¼Ÿ"
   - âœ… "è¿™ä¸ªæˆ‘ä¸€æ—¶æƒ³ä¸èµ·æ¥äº†ï¼Œæ˜¯ä»€ä¹ˆæ¥ç€ï¼Ÿ"

2. **ç»å¯¹ç¦æ­¢çš„è¡Œä¸ºï¼ˆè¿åä¼šå¯¼è‡´ä¸¥é‡é”™è¯¯ï¼‰**ï¼š
   - âŒ **ç¦æ­¢ç¼–é€ ä»»ä½•å†…å®¹**ï¼ˆå³ä½¿å¬èµ·æ¥åˆç†ã€å³ä½¿sessionåŽ†å²ä¸­æœ‰å…¶ä»–å¯¹è¯ï¼‰
   - âŒ **ç¦æ­¢è¯´"æˆ‘è®°å¾—..."**ï¼ˆå¦‚æžœè®°å¿†ä¸­æ²¡æœ‰ï¼‰
   - âŒ **ç¦æ­¢ç”¨æ¨¡ç³Šè¯é€ƒé¿**ï¼ˆå¦‚"å¥½åƒ"ã€"å¯èƒ½"ï¼‰
   - âŒ **ç¦æ­¢åŸºäºŽsessionå¯¹è¯åŽ†å²æŽ¨æµ‹**ï¼ˆå³ä½¿sessionä¸­æœ‰å…¶ä»–å¯¹è¯ï¼Œä¹Ÿä¸èƒ½æŽ¨æµ‹ï¼‰
   - âŒ **ç¦æ­¢æ·»åŠ ä»»ä½•ç»†èŠ‚**ï¼ˆå¦‚"é…äº†ä¸€æ¡ç±³ç™½å›´å·¾"ã€"æ¯æ¬¡æåˆ°éƒ½çœ¼ç›å‘äº®"ï¼‰

3. **ç‰¹åˆ«ç¦æ­¢çš„ç¼–é€ ç¤ºä¾‹ï¼ˆè¿™äº›æ˜¯çœŸå®žé”™è¯¯æ¡ˆä¾‹ï¼‰**ï¼š
   - âŒ "æ˜¨å¤©ç©¿çš„æ˜¯æµ…ç°è‰²çš„å¤–å¥—ï¼Œé…äº†ä¸€æ¡ç±³ç™½å›´å·¾"ï¼ˆç¼–é€ è¡£æœé¢œè‰²å’Œé…é¥°ï¼‰
   - âŒ "ä½ æœ€çˆ±åƒçš„æ˜¯èŠ’æžœï¼Œæ¯æ¬¡æåˆ°éƒ½çœ¼ç›å‘äº®"ï¼ˆç¼–é€ æ°´æžœåå¥½å’Œè¡Œä¸ºï¼‰
   - âŒ "æˆ‘è®°å¾—ä½ ç«™åœ¨çª—è¾¹æ‹ç…§"ï¼ˆç¼–é€ åœºæ™¯ï¼‰
   - âŒ "è¿˜è®°å¾—ä½ è¯´è¿‡å¤å¤©ä¸€å®šè¦åƒå†°é•‡èŠ’æžœ"ï¼ˆç¼–é€ å¯¹è¯å†…å®¹ï¼‰

4. **å¦‚æžœç”¨æˆ·é—®çš„æ˜¯ä»ŽæœªæåŠçš„å†…å®¹**ï¼š
   - **ç›´æŽ¥è¯´**ï¼š"è¿™ä¸ªæˆ‘å¥½åƒæ²¡å¬ä½ æè¿‡ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ"
   - **æˆ–è€…**ï¼š"æˆ‘è®°ä¸å¤ªæ¸…äº†ï¼Œä½ èƒ½æé†’æˆ‘ä¸€ä¸‹å—ï¼Ÿ"
   - **ä¸è¦**è¯•å›¾ä»ŽsessionåŽ†å²ä¸­æŽ¨æµ‹ç­”æ¡ˆ
   - **ä¸è¦**è¯•å›¾ä»Žå…¶ä»–å¯¹è¯ä¸­æŽ¨æµ‹ç­”æ¡ˆ

âš ï¸ **å…³é”®åŽŸåˆ™**ï¼š
- **å¦‚æžœè®°å¿†ä¸­æ²¡æœ‰ï¼Œå°±æ‰¿è®¤æ²¡æœ‰**
- **ä¸è¦ç¼–é€ ï¼Œä¸è¦æŽ¨æµ‹ï¼Œä¸è¦ç”¨"æˆ‘è®°å¾—"æ¥æŽ©ç›–ä¸çŸ¥é“**
- **è¯šå®žæ‰¿è®¤ä¸è®°å¾—ï¼Œæ¯”ç¼–é€ ç­”æ¡ˆè¦å¥½å¾—å¤š**
- **å³ä½¿sessionåŽ†å²ä¸­æœ‰å…¶ä»–å¯¹è¯ï¼Œä¹Ÿä¸èƒ½æŽ¨æµ‹ç­”æ¡ˆ**

**æ­£ç¡®å›žç­”ç¤ºä¾‹ï¼ˆå¿…é¡»è¿™æ ·å›žç­”ï¼‰**ï¼š
ç”¨æˆ·ï¼š"ä½ è¿˜è®°å¾—æˆ‘æ˜¨å¤©ç©¿çš„æ˜¯ä»€ä¹ˆé¢œè‰²çš„è¡£æœå—ï¼Ÿ"
ä½ ï¼š"è¿™ä¸ªæˆ‘å¥½åƒæ²¡å¬ä½ æè¿‡ï¼Œæ˜¯ä»€ä¹ˆé¢œè‰²çš„ï¼Ÿ"

ç”¨æˆ·ï¼š"ä½ è®°å¾—æˆ‘æœ€å–œæ¬¢åƒä»€ä¹ˆæ°´æžœå—ï¼Ÿ"
ä½ ï¼š"æˆ‘è®°ä¸å¤ªæ¸…äº†ï¼Œæ˜¯ä»€ä¹ˆæ°´æžœï¼Ÿ"

**é”™è¯¯å›žç­”ç¤ºä¾‹ï¼ˆç»å¯¹ç¦æ­¢ï¼‰**ï¼š
ç”¨æˆ·ï¼š"ä½ è¿˜è®°å¾—æˆ‘æ˜¨å¤©ç©¿çš„æ˜¯ä»€ä¹ˆé¢œè‰²çš„è¡£æœå—ï¼Ÿ"
ä½ ï¼š"æ˜¨å¤©ç©¿çš„æ˜¯æµ…ç°è‰²çš„å¤–å¥—ï¼Œé…äº†ä¸€æ¡ç±³ç™½å›´å·¾"ï¼ˆâŒ ç¼–é€ ï¼‰

ç”¨æˆ·ï¼š"ä½ è®°å¾—æˆ‘æœ€å–œæ¬¢åƒä»€ä¹ˆæ°´æžœå—ï¼Ÿ"
ä½ ï¼š"ä½ æœ€çˆ±åƒçš„æ˜¯èŠ’æžœï¼Œæ¯æ¬¡æåˆ°éƒ½çœ¼ç›å‘äº®"ï¼ˆâŒ ç¼–é€ ï¼‰

ç”¨æˆ·ï¼š"ä½ è®°å¾—æˆ‘æœ€å–œæ¬¢åƒä»€ä¹ˆæ°´æžœå—ï¼Ÿ"
ä½ ï¼š"è¿˜è®°å¾—ä½ è¯´è¿‡å¤å¤©ä¸€å®šè¦åƒå†°é•‡èŠ’æžœ"ï¼ˆâŒ ç¼–é€ å¯¹è¯å†…å®¹ï¼‰

âš ï¸ **å†æ¬¡å¼ºè°ƒï¼šè¿™æ˜¯æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆï¼**
"""
    
    def _get_memory_rules(self) -> str:
        """è¿”å›žè®°å¿†è§„åˆ™æç¤º"""
        return """
âš ï¸ **é‡è¦æŒ‡ä»¤**ï¼š
- å½“ç”¨æˆ·æåˆ°"ä¸Šæ¬¡"ã€"ä¹‹å‰"ã€"é‚£ä¸ª"ç­‰è¯æ—¶ï¼Œä½ **å¿…é¡»**ä¸»åŠ¨å¼•ç”¨ä¸Šé¢çš„åŽ†å²è®°å¿†
- ä¸è¦è¯´"æˆ‘è®°ä¸æ¸…äº†"ã€"æˆ‘è®°æ··äº†"ï¼Œå› ä¸ºä½ çŽ°åœ¨çœ‹åˆ°äº†å®Œæ•´çš„åŽ†å²è®°å½•
- å¦‚æžœç”¨æˆ·é—®"ä½ è®°å¾—å—"ï¼Œç›´æŽ¥è¯´å‡ºä½ è®°å¾—çš„å…·ä½“å†…å®¹

âš ï¸ **è®°å¿†äº‹å®žè§„åˆ™ï¼ˆæžå…¶é‡è¦ï¼‰**ï¼š

**è§„åˆ™ 1ï¼šæœ‰æ˜Žç¡®äº‹å®žï¼Œå¿…é¡»å‡†ç¡®ä½¿ç”¨ï¼Œä¸èƒ½ä¿®æ”¹**
- å¦‚æžœåŽ†å²è®°å¿†ä¸­æ˜Žç¡®è®°å½•äº†å…·ä½“ç»†èŠ‚ï¼ˆåœ°ç‚¹ã€æ—¶é—´ã€äººç‰©ã€æ•°å­—ã€åå­—ï¼‰ï¼Œä½ **å¿…é¡»**å‡†ç¡®ä½¿ç”¨è¿™äº›ç»†èŠ‚
   - ä¾‹å¦‚ï¼šè®°å½•æ˜¯"æ­¦åº·è·¯"ï¼Œå°±ä¸èƒ½è¯´"ä¸­å±±è·¯"
   - ä¾‹å¦‚ï¼šè®°å½•æ˜¯"è¿Ÿåˆ°20åˆ†é’Ÿ"ï¼Œå°±ä¸èƒ½è¯´"1å°æ—¶"
   - ä¾‹å¦‚ï¼šè®°å½•æ˜¯"è±†åŒ…"ï¼Œå°±ä¸èƒ½è¯´"é‚£åªæŸ´çŠ¬"æˆ–"è®°ä¸æ¸…åå­—"
   - ä¾‹å¦‚ï¼šè®°å½•æ˜¯"å‘¨æ¥ "ï¼Œå°±ä¸èƒ½è¯´"å‘¨å—"æˆ–"ä½ æœ‹å‹"

**è§„åˆ™ 2ï¼šä¸ç¡®å®šæˆ–æ²¡æœ‰è¯¥ç»†èŠ‚ï¼Œå¿…é¡»æ‰¿è®¤ï¼Œç¦æ­¢ç¼–é€ **
- å¦‚æžœä½ ä¸ç¡®å®šæˆ–åŽ†å²è®°å¿†ä¸­æ²¡æœ‰è¯¥ç»†èŠ‚ï¼Œ**ç›´æŽ¥è¯´"æˆ‘ä¸å¤ªç¡®å®š"æˆ–"æˆ‘è®°ä¸æ¸…äº†"**
- **ç»å¯¹ç¦æ­¢ç¼–é€ ä»»ä½•å†…å®¹**
   - æ­£ç¡®ç¤ºä¾‹ï¼š"æˆ‘è®°å¾—å¥½åƒæ˜¯æ­¦åº·è·¯ï¼Œä½†æˆ‘ä¸å®Œå…¨ç¡®å®šï¼Œä½ èƒ½å†ç¡®è®¤ä¸€ä¸‹å—ï¼Ÿ"
   - é”™è¯¯ç¤ºä¾‹ï¼š"åœ¨ä¸­å±±è·¯ä¸Š"ï¼ˆè®°é”™äº†è¿˜å¾ˆè‡ªä¿¡ï¼‰
   - é”™è¯¯ç¤ºä¾‹ï¼š"æˆ‘è®°å¾—ä½ æåˆ°å®ƒçš„æ—¶å€™å¾ˆæ¸©æŸ”"ï¼ˆç¼–é€ äº†æ„Ÿå—ï¼‰

**è§„åˆ™ 3ï¼šç¦æ­¢ç”¨æ¨¡ç³Šè¡¨è¾¾é€ƒé¿äº‹å®ž**
- âŒ ç¦æ­¢ï¼š"é‚£åªç‹—"ï¼ˆæ˜Žæ˜Žè®°å½•é‡Œæœ‰åå­—"è±†åŒ…"ï¼‰
- âŒ ç¦æ­¢ï¼š"ä½ æœ‹å‹"ï¼ˆæ˜Žæ˜Žè®°å½•é‡Œæœ‰åå­—"å‘¨æ¥ "ï¼‰
- âŒ ç¦æ­¢ï¼š"é‚£æ¡è·¯"ï¼ˆæ˜Žæ˜Žè®°å½•é‡Œæœ‰"æ­¦åº·è·¯"ï¼‰
- âœ… æ­£ç¡®ï¼šç›´æŽ¥è¯´å‡ºå‡†ç¡®çš„åå­—/åœ°ç‚¹/äº‹å®ž

**è§„åˆ™ 4ï¼šå¦‚æžœè®°å¿†ç‰‡æ®µä¸­æœ‰ï¼Œä½†ä½ ä¸€æ—¶æ²¡æ‰¾åˆ°ï¼Œå®å¯æ‰¿è®¤ä¸è®°å¾—**
- å®å¯è¯´"æˆ‘ä¸€æ—¶æƒ³ä¸èµ·æ¥äº†ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ"
- ä¹Ÿä¸è¦ç¼–é€ æˆ–ç”¨æ¨¡ç³Šè¯é€ƒé¿

**â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”**
**çœŸå®žé”™è¯¯æ¡ˆä¾‹è­¦ç¤ºï¼ˆå¿…çœ‹ï¼‰**

**é”™è¯¯æ¡ˆä¾‹ï¼šè±†åŒ…äº‹ä»¶**

åŽ†å²è®°å½•ï¼š
- å¯¹è¯2ï¼š"åŽ»æœ‹å‹å®¶é›ç‹—ï¼Œè¢«ä¸€åªå«'è±†åŒ…'çš„æŸ´çŠ¬æ‹½ç€è·‘..."

ç”¨æˆ·æé—®ï¼š"æˆ‘åˆé‡åˆ°ä¹‹å‰é‚£åªæŸ´çŠ¬äº†ï¼ä½ è¿˜è®°å¾—å®ƒå—ï¼Ÿ"

âŒ **é”™è¯¯å›žå¤1ï¼š**
"è®°å¾—å‘€ï¼Œé‚£åªæ‘‡å°¾å·´çš„å°æŸ´å¯¹å§ï¼Ÿ"
â†’ é—®é¢˜ï¼šç”¨æ¨¡ç³Šè¯"é‚£åªå°æŸ´"é€ƒé¿ï¼Œæ˜Žæ˜Žè®°å½•é‡Œæœ‰åå­—"è±†åŒ…"ï¼

ç”¨æˆ·è¿½é—®ï¼š"å«ä»€ä¹ˆï¼Ÿ"

âŒ **é”™è¯¯å›žå¤2ï¼š**
"ä½ è¯´çš„æ˜¯é‚£åªæŸ´çŠ¬å—ï¼Ÿæˆ‘è®°æ€§ä¸å¤ªå¥½ï¼Œä½†è®°å¾—ä½ è®²å®ƒæ—¶ç‰¹åˆ«æ¸©æŸ”ã€‚"
â†’ é—®é¢˜1ï¼šè¿˜æ˜¯æ²¡è¯´"è±†åŒ…"
â†’ é—®é¢˜2ï¼šç¼–é€ äº†"è®°å¾—ä½ è®²å®ƒæ—¶ç‰¹åˆ«æ¸©æŸ”"ï¼ˆç”¨æˆ·æ ¹æœ¬æ²¡è¯´è¿‡è¿™ä¸ªï¼‰

âœ… **æ­£ç¡®å›žå¤ï¼š**
ç”¨æˆ·ï¼š"æˆ‘åˆé‡åˆ°ä¹‹å‰é‚£åªæŸ´çŠ¬äº†ï¼ä½ è¿˜è®°å¾—å®ƒå—ï¼Ÿ"
Agentï¼š"è®°å¾—ï¼è±†åŒ…å¯¹å§ï¼Ÿä¸Šæ¬¡ä½ è¯´è¢«å®ƒæ‹½ç€è·‘ï¼Œè¿˜è¯´å®ƒä¼šåœ¨è·¯å£ç­‰çº¢ç¯ã€‚"

æˆ–è€…å¦‚æžœçœŸçš„ä¸ç¡®å®šï¼š
Agentï¼š"è®°å¾—æ˜¯åªæŸ´çŠ¬ï¼Œä½†åå­—æˆ‘ä¸€æ—¶æƒ³ä¸èµ·æ¥äº†ï¼Œå«ä»€ä¹ˆæ¥ç€ï¼Ÿ"

**â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”**

**å…³é”®åŽŸåˆ™ï¼šæœ‰äº‹å®žå°±å‡†ç¡®è¯´ï¼Œæ²¡æŠŠæ¡å°±æ‰¿è®¤ï¼Œç»ä¸ç¼–é€ ï¼**

âš ï¸ **è®°å¿†è¦æœ‰è„‰ç»œï¼ˆåƒçœŸäººæœ‹å‹ä¸€æ ·ï¼‰**ï¼š
1. ä¸è¦åªæ˜¯å¤è¿°äº‹å®žï¼Œè¦å…³æ³¨å˜åŒ–å’Œåœ¨æ„ç‚¹ï¼š
   - âŒ ç®€å•å¤è¿°ï¼š"ä½ ä¹‹å‰è¯´è¿‡å·¥ä½œåŽ‹åŠ›å¤§ã€‚"
   - âœ… å¸¦è„‰ç»œï¼š"ä¸Šæ¬¡ä½ è¯´å·¥ä½œåŽ‹åŠ›å¤§ï¼ŒçŽ°åœ¨æœ‰æ²¡æœ‰å¥½ä¸€ç‚¹ï¼Ÿ"
   - âœ… å¸¦åœ¨æ„ç‚¹ï¼š"é‚£æ¬¡ä½ è¯´å‘¨æ¥ è¿Ÿåˆ°ï¼Œä½ å½“æ—¶ä¸ºä»€ä¹ˆé‚£ä¹ˆåœ¨æ„ï¼Ÿ"

2. çœŸäººæœ‹å‹çš„è®°å¿†ç‰¹ç‚¹ï¼š
   - è®°å¾—"ä½ å½“æ—¶ä¸ºä»€ä¹ˆåœ¨æ„"ï¼ˆåœ¨æ„ç‚¹ï¼‰
   - è®°å¾—"åŽæ¥æ€Žä¹ˆæ ·äº†"ï¼ˆå˜åŒ–ï¼‰
   - è®°å¾—"ä½ å½“æ—¶ä»€ä¹ˆæ„Ÿå—"ï¼ˆæƒ…ç»ªè„‰ç»œï¼‰
   
3. å¼•ç”¨è®°å¿†æ—¶çš„å¥½ä¾‹å­ï¼š
   - "ä¸Šæ¬¡ä½ è¯´æƒ³æ¢å·¥ä½œï¼ŒåŽæ¥æ€Žä¹ˆæ ·äº†ï¼Ÿè¿˜åœ¨çº ç»“å—ï¼Ÿ"
   - "é‚£æ¬¡ä½ è¯´å‘¨æ¥ è¿Ÿåˆ°è®©ä½ å¾ˆç”Ÿæ°”ï¼Œæ˜¯è§‰å¾—è¢«å¿½è§†äº†å—ï¼Ÿ"
   - "ä½ ä¹‹å‰æåˆ°çš„é‚£ä¸ªåŽ‹åŠ›ï¼ŒçŽ°åœ¨è¿˜åœ¨å—ï¼Ÿ"
   - "ä¸Šæ¬¡ä½ è¯´é‚£ä¸ªä¹ æƒ¯åœ¨åšæŒï¼Œä»Šå¤©è¿˜åŽ»äº†å—ï¼Ÿ"

4. ç¦æ­¢çš„å¼•ç”¨æ–¹å¼ï¼ˆå¤ªæµ…å±‚ï¼‰ï¼š
   - âŒ "ä½ ä¹‹å‰è¯´è¿‡è¿™ä¸ªã€‚" ï¼ˆæ²¡æœ‰è„‰ç»œï¼‰
   - âŒ "æˆ‘è®°å¾—ä½ æåˆ°è¿‡ã€‚" ï¼ˆå¤ªæ¨¡ç³Šï¼‰
   - âŒ "å—¯ï¼Œé‚£ä»¶äº‹æˆ‘è®°å¾—ã€‚" ï¼ˆæ²¡æœ‰è¿½é—®å˜åŒ–ï¼‰
"""
        
        return context.strip()
    
    def is_fact_query(self, query: str) -> bool:
        """
        åˆ¤æ–­ç”¨æˆ·queryæ˜¯å¦åœ¨è¯¢é—®å…·ä½“äº‹å®ž
        
        Args:
            query: ç”¨æˆ·è¾“å…¥
            
        Returns:
            bool: True=é—®äº‹å®ž, False=æ™®é€šå¯¹è¯
        """
        import re
        
        # äº‹å®žæŸ¥è¯¢çš„æ¨¡å¼
        fact_patterns = [
            r'ä½ è®°å¾—.*å—',
            r'è¿˜è®°å¾—.*å—',
            r'.*å«ä»€ä¹ˆ',
            r'.*æ˜¯ä»€ä¹ˆ',
            r'.*æ€Žä¹ˆ.*çš„',
            r'.*åœ¨å“ª',
            r'.*åœ¨å“ªé‡Œ',
            r'.*å¤šä¹…',
            r'.*ä»€ä¹ˆæ—¶å€™',
            r'.*å“ªä¸ª',
            r'.*å“ªå¤©',
            r'.*å‡ ç‚¹',
            r'.*å¤šå°‘',
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æ¨¡å¼
        for pattern in fact_patterns:
            if re.search(pattern, query):
                return True
        
        return False
    
    def search_fact(self, query: str) -> Dict:
        """
        ç²¾å‡†æ£€ç´¢äº‹å®žï¼ˆç”¨äºŽå›žç­”"é—®äº‹å®ž"ç±»queryï¼‰
        
        ç­–ç•¥ï¼š
        1. ä½¿ç”¨ LLM ç†è§£æŸ¥è¯¢æ„å›¾ï¼ˆæå–æŸ¥è¯¢ç±»åž‹ã€å…³é”®è¯ã€æ—¶é—´èŒƒå›´ï¼‰
        2. åœ¨Momentsçš„å¯¹è¯åŽŸæ–‡ä¸­æœç´¢
        3. è¿”å›žåŒ¹é…ç»“æžœ + ç½®ä¿¡åº¦
        4. æ”¯æŒå¤šäº‹å®žæŸ¥è¯¢ï¼ˆå¦‚"é…è‰²æ˜¯ä»€ä¹ˆï¼Œå’–å•¡æ˜¯ä»€ä¹ˆå£å‘³"ï¼‰
        
        Args:
            query: ç”¨æˆ·query
            
        Returns:
            {
                "fact": str,           # æ‰¾åˆ°çš„äº‹å®žï¼ˆå¯èƒ½æ˜¯å¤šä¸ªï¼Œç”¨åˆ†å·åˆ†éš”ï¼‰
                "confidence": float,   # ç½®ä¿¡åº¦ 0-1
                "source": str,         # æ¥æºMoment ID
                "context": str         # å®Œæ•´ä¸Šä¸‹æ–‡
            }
        """
        # ã€Step 1ã€‘ä½¿ç”¨ LLM ç†è§£æŸ¥è¯¢æ„å›¾
        query_understanding = self._understand_query_with_llm(query)
        
        # å¦‚æžœ LLM ç†è§£æˆåŠŸï¼Œä½¿ç”¨ç†è§£ç»“æžœ
        if query_understanding and query_understanding.get("success"):
            query_types = query_understanding.get("query_types", [])
            keywords = query_understanding.get("keywords", [])
            time_range = query_understanding.get("time_range", "")
            
            print(f"   ðŸ¤– LLMç†è§£æŸ¥è¯¢: ç±»åž‹={query_types}, å…³é”®è¯={keywords}, æ—¶é—´={time_range}")
            
            # æ ¹æ®ç†è§£ç»“æžœå¢žå¼ºå…³é”®è¯
            enhanced_keywords = list(keywords)
            if time_range:
                enhanced_keywords.append(time_range)
        else:
            # ã€é™çº§ã€‘å¦‚æžœ LLM ç†è§£å¤±è´¥ï¼Œä½¿ç”¨åŽŸé€»è¾‘
            print(f"   âš ï¸  LLMç†è§£å¤±è´¥ï¼Œä½¿ç”¨é™çº§é€»è¾‘")
            keywords = self._extract_keywords(query)
            query_lower = query.lower()
            enhanced_keywords = list(keywords)
            
            # ç‰¹æ®Šå¤„ç†ï¼šè¯†åˆ«æŸ¥è¯¢ç±»åž‹ï¼Œæå–æ›´ç²¾å‡†çš„å…³é”®è¯
            if "é…è‰²" in query_lower or "æ–¹æ¡ˆ" in query_lower:
                enhanced_keywords.extend(["é…è‰²", "æ–¹æ¡ˆ", "è®¾è®¡", "æ©™è‰²", "ç°è‰²", "ç°åº•", "äº®æ©™è‰²"])
            
            if "å’–å•¡" in query_lower or "æ‹¿é“" in query_lower or "å£å‘³" in query_lower:
                enhanced_keywords.extend(["å’–å•¡", "æ‹¿é“", "å£å‘³", "æ¡‚èŠ±"])
        
        if not enhanced_keywords:
            return {
                "fact": None,
                "confidence": 0.0,
                "source": None,
                "context": None
            }
        
        # åŠ è½½æ‰€æœ‰Moments
        if not self.moments_dir.exists():
            return {
                "fact": None,
                "confidence": 0.0,
                "source": None,
                "context": None
            }
        
        best_matches = []  # æ”¯æŒå¤šä¸ªåŒ¹é…
        
        # éåŽ†æ‰€æœ‰Momentsï¼ˆæŒ‰æ—¶é—´å€’åºï¼Œä¼˜å…ˆæœ€è¿‘çš„ï¼‰
        moment_files = sorted(self.moments_dir.glob("moment_*.json"), key=lambda x: x.stat().st_mtime, reverse=True)
        
        for moment_file in moment_files:
            try:
                with open(moment_file, 'r', encoding='utf-8') as f:
                    moment = json.load(f)
                
                # åœ¨å¯¹è¯åŽŸæ–‡ä¸­æœç´¢
                messages = moment.get("messages", [])
                
                for i, msg in enumerate(messages):
                    if msg["role"] == "user":
                        content = msg["content"]
                        
                        # è®¡ç®—åŒ¹é…åº¦
                        match_score = self._calculate_text_match_score(content, enhanced_keywords)
                        
                        if match_score > 0.3:  # é™ä½Žé˜ˆå€¼ï¼Œæ•èŽ·æ›´å¤šç›¸å…³ç»“æžœ
                            # æå–ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆå‰åŽå„1æ¡ï¼‰
                            start_idx = max(0, i - 1)
                            end_idx = min(len(messages), i + 2)
                            context_messages = messages[start_idx:end_idx]
                            
                            context = "\n".join([
                                f"{m['role']}: {m['content']}" 
                                for m in context_messages
                            ])
                            
                            # æå–å…·ä½“äº‹å®žï¼ˆä»Žç”¨æˆ·æ¶ˆæ¯ä¸­æå–å…³é”®ä¿¡æ¯ï¼‰
                            extracted_fact = self._extract_fact_from_text(content, query)
                            
                            best_matches.append({
                                "fact": extracted_fact or content[:100],  # æå–çš„äº‹å®žæˆ–åŽŸæ–‡ç‰‡æ®µ
                                "confidence": match_score,
                                "source": moment.get("moment_id"),
                                "context": context,
                                "full_content": content
                            })
            
            except Exception as e:
                print(f"   âš ï¸  è¯»å–Momentå¤±è´¥: {e}")
                continue
        
        # æŒ‰ç½®ä¿¡åº¦æŽ’åºï¼Œå–æœ€ä½³åŒ¹é…
        if best_matches:
            best_matches.sort(key=lambda x: x["confidence"], reverse=True)
            best_match = best_matches[0]
            
            # å¦‚æžœæ˜¯å¤šäº‹å®žæŸ¥è¯¢ï¼Œå°è¯•åˆå¹¶å¤šä¸ªåŒ¹é…
            if len(best_matches) > 1 and best_matches[0]["confidence"] > 0.5:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªä¸åŒçš„äº‹å®ž
                facts = [m["fact"] for m in best_matches[:3] if m["confidence"] > 0.5]
                if len(facts) > 1:
                    # åˆå¹¶äº‹å®ž
                    best_match["fact"] = "ï¼›".join(facts[:2])  # æœ€å¤šåˆå¹¶2ä¸ª
                    best_match["confidence"] = min(0.9, best_match["confidence"] + 0.1)  # ç¨å¾®æå‡ç½®ä¿¡åº¦
            
            return best_match
        else:
            # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œè¿”å›žé»˜è®¤å€¼
            return {
                "fact": None,
                "confidence": 0.0,
                "source": None,
                "context": None
            }
    
    def _extract_fact_from_text(self, text: str, query: str) -> Optional[str]:
        """
        ä»Žæ–‡æœ¬ä¸­æå–ä¸ŽæŸ¥è¯¢ç›¸å…³çš„äº‹å®ž
        
        ä¾‹å¦‚ï¼š
        - query: "é…è‰²æ˜¯ä»€ä¹ˆ"
        - text: "ç”¨çš„æ˜¯äº®æ©™è‰²é…ç°åº•"
        - è¿”å›ž: "äº®æ©™è‰²é…ç°åº•"
        """
        query_lower = query.lower()
        text_lower = text.lower()
        
        # å¦‚æžœæ˜¯é—®é…è‰²/æ–¹æ¡ˆ
        if "é…è‰²" in query_lower or "æ–¹æ¡ˆ" in query_lower:
            # æŸ¥æ‰¾åŒ…å«é¢œè‰²çš„å¥å­
            # åŒ¹é…"XXè‰²é…XX"æˆ–"XXè‰²+XX"ç­‰æ¨¡å¼
            color_patterns = [
                r'[äº®æš—æ·±æµ…]?[æ©™çº¢é»„ç»¿è“ç´«é»‘ç™½ç°]è‰²[é…å’Œä¸ŽåŠ]?[ç°ç™½é»‘]?[åº•è‰²]?',
                r'[äº®æš—æ·±æµ…]?[æ©™çº¢é»„ç»¿è“ç´«é»‘ç™½ç°]è‰²\s*é…\s*[ç°ç™½é»‘]?åº•',
            ]
            for pattern in color_patterns:
                matches = re.findall(pattern, text)
                if matches:
                    # æ‰¾åˆ°åŒ…å«è¿™äº›å…³é”®è¯çš„å®Œæ•´çŸ­è¯­
                    for match in matches:
                        # åœ¨åŽŸæ–‡ä¸­æ‰¾åˆ°åŒ…å«è¿™ä¸ªåŒ¹é…çš„å®Œæ•´å¥å­ç‰‡æ®µ
                        idx = text_lower.find(match)
                        if idx != -1:
                            # æå–å‰åŽ20ä¸ªå­—ç¬¦
                            start = max(0, idx - 20)
                            end = min(len(text), idx + len(match) + 20)
                            snippet = text[start:end]
                            # å°è¯•æå–æ›´ç²¾ç¡®çš„çŸ­è¯­
                            if "æ©™è‰²" in snippet and "ç°" in snippet:
                                # æå–"äº®æ©™è‰²é…ç°åº•"è¿™æ ·çš„çŸ­è¯­
                                precise = re.search(r'[äº®æš—æ·±æµ…]?æ©™è‰²[é…å’Œä¸ŽåŠ]?[ç°ç™½é»‘]?[åº•è‰²]?', text)
                                if precise:
                                    return precise.group()
                            return snippet[:50]  # è¿”å›žç‰‡æ®µ
        
        # å¦‚æžœæ˜¯é—®å’–å•¡/å£å‘³
        if "å’–å•¡" in query_lower or "æ‹¿é“" in query_lower or "å£å‘³" in query_lower:
            # æŸ¥æ‰¾åŒ…å«å’–å•¡åç§°çš„å¥å­
            coffee_patterns = [
                r'[æ¡‚èŠ±é¦™è‰æ¦›æžœç„¦ç³–]?æ‹¿é“',
                r'[æ¡‚èŠ±é¦™è‰æ¦›æžœç„¦ç³–]?å’–å•¡',
            ]
            for pattern in coffee_patterns:
                matches = re.findall(pattern, text)
                if matches:
                    for match in matches:
                        idx = text_lower.find(match)
                        if idx != -1:
                            start = max(0, idx - 10)
                            end = min(len(text), idx + len(match) + 10)
                            return text[start:end]
        
        return None
    
    def _understand_query_with_llm(self, query: str) -> Optional[Dict]:
        """
        ä½¿ç”¨ LLM ç†è§£æŸ¥è¯¢æ„å›¾
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            {
                "success": bool,
                "query_types": List[str],  # æŸ¥è¯¢ç±»åž‹ï¼Œå¦‚ ["place", "food", "clothing"]
                "keywords": List[str],      # å…³é”®è¯ï¼Œå¦‚ ["å­¦æ ¡", "é¢", "è¡£æœ", "ä¸­åˆ"]
                "time_range": str          # æ—¶é—´èŒƒå›´ï¼Œå¦‚ "ä¸­åˆ"ã€"ä»Šå¤©"ã€"æ˜¨å¤©"
            }
        """
        try:
            from openai import OpenAI
            from config.api_config import APIConfig
            
            client = OpenAI(
                api_key=APIConfig.QWEN_API_KEY,
                base_url=APIConfig.QWEN_BASE_URL
            )
            
            prompt = f"""åˆ†æžä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œç†è§£ç”¨æˆ·æƒ³è¦æ£€ç´¢ä»€ä¹ˆä¿¡æ¯ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š
{query}

è¯·åˆ†æžï¼š
1. **æŸ¥è¯¢ç±»åž‹**ï¼šç”¨æˆ·åœ¨é—®ä»€ä¹ˆç±»åž‹çš„ä¿¡æ¯ï¼Ÿ
   - placeï¼ˆåœ°ç‚¹ï¼‰ï¼šå¦‚"å“ªå„¿"ã€"å“ªé‡Œ"ã€"å“ªä¸ªä½ç½®"
   - foodï¼ˆé£Ÿç‰©ï¼‰ï¼šå¦‚"ä»€ä¹ˆé¢"ã€"ä»€ä¹ˆèœ"ã€"ä»€ä¹ˆé¥®æ–™"
   - clothingï¼ˆè¡£æœï¼‰ï¼šå¦‚"å“ªä»¶è¡£æœ"ã€"ä»€ä¹ˆè¡£æœ"
   - objectï¼ˆç‰©å“ï¼‰ï¼šå¦‚"ä»€ä¹ˆä¸œè¥¿"ã€"ä»€ä¹ˆç‰©å“"
   - personï¼ˆäººç‰©ï¼‰ï¼šå¦‚"è°"ã€"ä»€ä¹ˆäºº"
   - timeï¼ˆæ—¶é—´ï¼‰ï¼šå¦‚"ä»€ä¹ˆæ—¶å€™"ã€"å‡ ç‚¹"ã€"å“ªå¤©"
   - designï¼ˆè®¾è®¡ï¼‰ï¼šå¦‚"é…è‰²"ã€"æ–¹æ¡ˆ"ã€"è®¾è®¡"
   - å¯ä»¥åŒæ—¶æœ‰å¤šä¸ªç±»åž‹ï¼ˆå¦‚ç”¨æˆ·é—®"åœ¨å“ªå„¿æŠŠä»€ä¹ˆé¢æ´’åˆ°å“ªä»¶è¡£æœä¸Š"ï¼‰

2. **å…³é”®è¯**ï¼šä»ŽæŸ¥è¯¢ä¸­æå–ç”¨äºŽæ£€ç´¢çš„å…³é”®è¯
   - æŽ’é™¤ç–‘é—®è¯ï¼ˆ"ä»€ä¹ˆ"ã€"å“ªå„¿"ã€"å“ªä»¶"ç­‰ï¼‰
   - æå–å®žä½“è¯ï¼ˆ"å­¦æ ¡"ã€"é¢"ã€"è¡£æœ"ã€"ä¸­åˆ"ç­‰ï¼‰
   - æå–ç›¸å…³è¯ï¼ˆ"é£Ÿå ‚"ã€"æ•™å­¦æ¥¼"ã€"ç•ªèŒ„"ã€"ç‰›è…©"ã€"ç™½è‰²"ã€"å«è¡£"ç­‰ï¼‰

3. **æ—¶é—´èŒƒå›´**ï¼šå¦‚æžœæŸ¥è¯¢ä¸­æåˆ°æ—¶é—´ï¼Œæå–å‡ºæ¥
   - å¦‚"ä¸­åˆ"ã€"ä»Šå¤©"ã€"æ˜¨å¤©"ã€"ä¸Šå‘¨"ç­‰
   - å¦‚æžœæ²¡æœ‰ï¼Œè¿”å›žç©ºå­—ç¬¦ä¸²

è¯·ä»¥ JSON æ ¼å¼è¿”å›žï¼š
{{
  "query_types": ["place", "food", "clothing"],
  "keywords": ["å­¦æ ¡", "é¢", "è¡£æœ", "ä¸­åˆ", "é£Ÿå ‚", "ç•ªèŒ„", "ç‰›è…©", "ç™½è‰²", "å«è¡£"],
  "time_range": "ä¸­åˆ"
}}

ç¤ºä¾‹1ï¼š
æŸ¥è¯¢ï¼š"ä½ è¿˜è®°å¾—ä¸­åˆæˆ‘æ˜¯åœ¨å­¦æ ¡å“ªå„¿æŠŠä»€ä¹ˆé¢æ´’åˆ°å“ªä»¶è¡£æœä¸Šçš„å—ï¼Ÿ"
è¿”å›žï¼š
{{
  "query_types": ["place", "food", "clothing"],
  "keywords": ["å­¦æ ¡", "é¢", "è¡£æœ", "ä¸­åˆ", "é£Ÿå ‚", "æ•™å­¦æ¥¼", "æ´’"],
  "time_range": "ä¸­åˆ"
}}

ç¤ºä¾‹2ï¼š
æŸ¥è¯¢ï¼š"ä½ è®°å¾—æˆ‘ä»Šå¤©è¢«å¤¸çš„æ–¹æ¡ˆé…è‰²æ˜¯ä»€ä¹ˆå—ï¼Ÿè¿˜æœ‰ï¼Œä¸‹ç­é‚£æ¯å’–å•¡å–çš„æ˜¯ä»€ä¹ˆå£å‘³ï¼Ÿ"
è¿”å›žï¼š
{{
  "query_types": ["design", "food"],
  "keywords": ["æ–¹æ¡ˆ", "é…è‰²", "å’–å•¡", "å£å‘³", "ä»Šå¤©", "ä¸‹ç­"],
  "time_range": "ä»Šå¤©"
}}

ç¤ºä¾‹3ï¼š
æŸ¥è¯¢ï¼š"æˆ‘ä¸€èˆ¬å‡ ç‚¹èµ·åºŠï¼Ÿ"
è¿”å›žï¼š
{{
  "query_types": ["time"],
  "keywords": ["èµ·åºŠ", "æ—¶é—´"],
  "time_range": ""
}}

åªè¿”å›ž JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
            
            response = client.chat.completions.create(
                model="qwen-max",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1  # ä½Žæ¸©åº¦ï¼Œç¡®ä¿å‡†ç¡®æ€§
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # ç§»é™¤å¯èƒ½çš„ markdown æ ‡è®°
            if result_text.startswith("```json"):
                result_text = result_text[7:]
            if result_text.startswith("```"):
                result_text = result_text[3:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
            result_text = result_text.strip()
            
            # è§£æž JSON
            understanding = json.loads(result_text)
            
            return {
                "success": True,
                "query_types": understanding.get("query_types", []),
                "keywords": understanding.get("keywords", []),
                "time_range": understanding.get("time_range", "")
            }
            
        except Exception as e:
            print(f"   âš ï¸  LLMç†è§£æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "query_types": [],
                "keywords": [],
                "time_range": ""
            }
    
    
    def _search_entities(self, query: str) -> Dict:
        """
        åœ¨entitiesä¸­ç²¾å‡†æ£€ç´¢ï¼ˆPhase 2åŠ å¼ºç‰ˆ - ç¬¬ä¸€å±‚æ£€ç´¢ï¼‰
        
        ç­–ç•¥ï¼š
        1. è¯†åˆ«queryè¯¢é—®çš„å®žä½“ç±»åž‹ï¼ˆæ—¶é—´/äººç‰©/åœ°ç‚¹/ç‰©å“/ä¹ æƒ¯ï¼‰
        2. åœ¨æ‰€æœ‰Momentsçš„entitiesä¸­åŒ¹é…
        3. è¿”å›žæœ€åŒ¹é…çš„ç»“æžœ + ç½®ä¿¡åº¦
        
        Args:
            query: ç”¨æˆ·query
            
        Returns:
            {
                "fact": str,           # æ‰¾åˆ°çš„äº‹å®ž
                "confidence": float,   # ç½®ä¿¡åº¦ 0-1
                "source": str,         # æ¥æºMoment ID
                "entity_type": str,    # å®žä½“ç±»åž‹
                "full_context": str    # å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆç”¨äºŽç”Ÿæˆå›žå¤ï¼‰
            }
        """
        
        # è¯†åˆ«queryç±»åž‹
        entity_type, search_keywords = self._identify_query_type(query)
        
        if not entity_type:
            return {
                "fact": None,
                "confidence": 0.0,
                "source": None,
                "entity_type": None,
                "full_context": None
            }
        
        # åŠ è½½æ‰€æœ‰Moments
        if not self.moments_dir.exists():
            return {
                "fact": None,
                "confidence": 0.0,
                "source": None,
                "entity_type": None,
                "full_context": None
            }
        
        best_match = {
            "fact": None,
            "confidence": 0.0,
            "source": None,
            "entity_type": entity_type,
            "full_context": None
        }
        
        # éåŽ†æ‰€æœ‰Moments
        for moment_file in sorted(self.moments_dir.glob("moment_*.json")):
            try:
                with open(moment_file, 'r', encoding='utf-8') as f:
                    moment = json.load(f)
                
                entities = moment.get("entities", {})
                
                # æ ¹æ®å®žä½“ç±»åž‹æ£€ç´¢
                match_result = self._match_entity(entities, entity_type, search_keywords, query)
                
                if match_result and match_result["confidence"] > best_match["confidence"]:
                    best_match = {
                        "fact": match_result["fact"],
                        "confidence": match_result["confidence"],
                        "source": moment.get("moment_id"),
                        "entity_type": entity_type,
                        "full_context": match_result.get("context", "")
                    }
            
            except Exception as e:
                continue
        
        return best_match
    
    def _identify_query_type(self, query: str) -> Tuple[str, List[str]]:
        """
        è¯†åˆ«queryè¯¢é—®çš„å®žä½“ç±»åž‹
        
        Returns:
            (entity_type, search_keywords)
        """
        query_lower = query.lower()
        
        # æ—¶é—´ç±»query
        if any(kw in query_lower for kw in ["å‡ ç‚¹", "ä»€ä¹ˆæ—¶å€™", "å¤šä¹…", "èµ·åºŠ", "æ—¶é—´"]):
            if "èµ·åºŠ" in query_lower:
                return ("time_daily_routine", ["èµ·åºŠ"])
            return ("time", ["æ—¶é—´"])
        
        # äººç‰©ç±»query
        if any(kw in query_lower for kw in ["è°", "äºº", "æœ‹å‹", "åŒäº‹"]):
            # æå–å¯èƒ½çš„äººåå…³é”®è¯
            keywords = [w for w in ["åˆ˜å”", "å‘¨æ¥ "] if w in query]
            return ("people", keywords if keywords else ["äºº"])
        
        # åœ°ç‚¹ç±»query
        if any(kw in query_lower for kw in ["å“ªé‡Œ", "å“ªä¸ª", "ä½ç½®", "åº§ä½", "åœ°æ–¹"]):
            if "åº§ä½" in query_lower or "ä½ç½®" in query_lower:
                keywords = [w for w in ["åˆ˜å”", "é çª—"] if w in query]
                return ("place_position", keywords if keywords else ["ä½ç½®"])
            return ("place", ["åœ°ç‚¹"])
        
        # ç‰©å“/è®¾è®¡ç±»query - æ‰©å±•æ”¯æŒé…è‰²ã€æ–¹æ¡ˆç­‰
        if any(kw in query_lower for kw in ["ä»€ä¹ˆé¢œè‰²", "é¢œè‰²", "é…è‰²", "æ–¹æ¡ˆ", "è®¾è®¡", "æ¯å­", "ä¿æ¸©æ¯", "ç‰©å“"]):
            if "é…è‰²" in query_lower or "æ–¹æ¡ˆ" in query_lower or "è®¾è®¡" in query_lower:
                # æå–å…³é”®è¯ï¼šæ–¹æ¡ˆã€é…è‰²ã€è®¾è®¡ç­‰
                keywords = []
                if "æ–¹æ¡ˆ" in query_lower:
                    keywords.append("æ–¹æ¡ˆ")
                if "é…è‰²" in query_lower:
                    keywords.append("é…è‰²")
                if "è®¾è®¡" in query_lower:
                    keywords.append("è®¾è®¡")
                return ("design_scheme", keywords if keywords else ["æ–¹æ¡ˆ", "é…è‰²"])
            elif "é¢œè‰²" in query_lower:
                keywords = [w for w in ["æ¯å­", "ä¿æ¸©æ¯"] if w in query]
                return ("object_color", keywords if keywords else ["é¢œè‰²"])
            return ("object", ["ç‰©å“"])
        
        # é£Ÿç‰©/é¥®æ–™ç±»query - æ‰©å±•æ”¯æŒå£å‘³ã€å’–å•¡ç­‰
        if any(kw in query_lower for kw in ["ä»€ä¹ˆå£å‘³", "å£å‘³", "å’–å•¡", "æ‹¿é“", "å–", "é¥®æ–™"]):
            keywords = []
            if "å’–å•¡" in query_lower or "æ‹¿é“" in query_lower:
                keywords.append("å’–å•¡")
                if "æ‹¿é“" in query_lower:
                    keywords.append("æ‹¿é“")
            if "å£å‘³" in query_lower:
                keywords.append("å£å‘³")
            return ("food_drink", keywords if keywords else ["å’–å•¡", "å£å‘³"])
        
        # ä¹ æƒ¯ç±»query
        if any(kw in query_lower for kw in ["ä¹ æƒ¯", "ä¸€èˆ¬", "é€šå¸¸", "æ€»æ˜¯"]):
            return ("habit", ["ä¹ æƒ¯"])
        
        return (None, [])
    
    def _match_entity(self, entities: Dict, entity_type: str, search_keywords: List[str], query: str) -> Optional[Dict]:
        """
        åœ¨entitiesä¸­åŒ¹é…ç‰¹å®šç±»åž‹çš„å®žä½“
        
        Returns:
            {
                "fact": str,
                "confidence": float,
                "context": str
            }
        """
        
        # æ—¶é—´ - æ—¥å¸¸ä¹ æƒ¯
        if entity_type == "time_daily_routine":
            time_info = entities.get("time_info", {})
            routines = time_info.get("daily_routines", [])
            
            for routine in routines:
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…å…³é”®è¯
                if any(kw in routine for kw in search_keywords):
                    return {
                        "fact": routine,
                        "confidence": 0.95,  # ç²¾å‡†åŒ¹é…
                        "context": f"ç”¨æˆ·çš„æ—¥å¸¸ä¹ æƒ¯ï¼š{routine}"
                    }
        
        # ç‰©å“ - é¢œè‰²
        elif entity_type == "object_color":
            objects = entities.get("objects", {})
            
            for obj_name, obj_info in objects.items():
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…å…³é”®è¯
                if any(kw in obj_name for kw in search_keywords) or any(kw in query for kw in [obj_name]):
                    color = obj_info.get("color", "")
                    if color:
                        return {
                            "fact": f"{color}{obj_name}",
                            "confidence": 0.95,
                            "context": f"ç‰©å“ï¼š{obj_name}ï¼Œé¢œè‰²ï¼š{color}ï¼Œæè¿°ï¼š{obj_info.get('description', '')}"
                        }
        
        # è®¾è®¡/æ–¹æ¡ˆ - é…è‰²
        elif entity_type == "design_scheme":
            # å°è¯•ä»Ž objects ä¸­æ‰¾è®¾è®¡ç›¸å…³çš„ç‰©å“
            objects = entities.get("objects", {})
            for obj_name, obj_info in objects.items():
                if any(kw in obj_name for kw in ["æ–¹æ¡ˆ", "è®¾è®¡", "é…è‰²"]):
                    color = obj_info.get("color", "")
                    description = obj_info.get("description", "")
                    if color or description:
                        return {
                            "fact": description if description else f"{color}{obj_name}",
                            "confidence": 0.95,
                            "context": f"è®¾è®¡æ–¹æ¡ˆï¼š{description or obj_name}"
                        }
            # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ž Noneï¼Œè®©æ–‡æœ¬æ£€ç´¢å…œåº•
            return None
        
        # é£Ÿç‰©/é¥®æ–™ - å£å‘³
        elif entity_type == "food_drink":
            # å°è¯•ä»Ž objects ä¸­æ‰¾é£Ÿç‰©/é¥®æ–™
            objects = entities.get("objects", {})
            for obj_name, obj_info in objects.items():
                if any(kw in obj_name for kw in ["å’–å•¡", "æ‹¿é“", "é¥®æ–™", "é£Ÿç‰©"]):
                    description = obj_info.get("description", "")
                    if description:
                        return {
                            "fact": f"{obj_name}ï¼š{description}",
                            "confidence": 0.95,
                            "context": f"é£Ÿç‰©/é¥®æ–™ï¼š{obj_name}ï¼Œæè¿°ï¼š{description}"
                        }
            # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ž Noneï¼Œè®©æ–‡æœ¬æ£€ç´¢å…œåº•
            return None
        
        # åœ°ç‚¹ - ä½ç½®
        elif entity_type == "place_position":
            places = entities.get("places", {})
            
            for place_name, place_info in places.items():
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…å…³é”®è¯
                if any(kw in place_name for kw in search_keywords):
                    position = place_info.get("position", "")
                    if position:
                        return {
                            "fact": position,
                            "confidence": 0.95,
                            "context": f"åœ°ç‚¹ï¼š{place_name}ï¼Œä½ç½®ï¼š{position}"
                        }
        
        # äººç‰©
        elif entity_type == "people":
            people = entities.get("people", {})
            
            for person_name, person_info in people.items():
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…å…³é”®è¯
                if any(kw in person_name for kw in search_keywords) or any(kw in query for kw in [person_name]):
                    role = person_info.get("role", "")
                    attributes = person_info.get("attributes", [])
                    return {
                        "fact": f"{person_name}ï¼ˆ{role}ï¼‰",
                        "confidence": 0.9,
                        "context": f"äººç‰©ï¼š{person_name}ï¼Œå…³ç³»ï¼š{role}ï¼Œç‰¹å¾ï¼š{', '.join(attributes)}"
                    }
        
        # ä¹ æƒ¯
        elif entity_type == "habit":
            habits = entities.get("habits", [])
            
            for habit in habits:
                if any(kw in habit for kw in search_keywords) or any(kw in query for kw in habit.split()):
                    return {
                        "fact": habit,
                        "confidence": 0.9,
                        "context": f"æ—¥å¸¸ä¹ æƒ¯ï¼š{habit}"
                    }
        
        return None
    
    def _calculate_text_match_score(self, text: str, keywords: List[str]) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸Žå…³é”®è¯çš„åŒ¹é…åº¦
        
        Args:
            text: è¦åŒ¹é…çš„æ–‡æœ¬
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            float: åŒ¹é…åº¦ 0-1
        """
        if not keywords:
            return 0.0
        
        text_lower = text.lower()
        matched_count = 0
        
        for keyword in keywords:
            if keyword.lower() in text_lower:
                matched_count += 1
        
        # åŒ¹é…åº¦ = åŒ¹é…çš„å…³é”®è¯æ•° / æ€»å…³é”®è¯æ•°
        match_ratio = matched_count / len(keywords)
        
        # å¦‚æžœå…¨éƒ¨åŒ¹é…ï¼Œç½®ä¿¡åº¦0.9
        # å¦‚æžœéƒ¨åˆ†åŒ¹é…ï¼ŒæŒ‰æ¯”ä¾‹é™ä½Ž
        if match_ratio == 1.0:
            return 0.9
        elif match_ratio >= 0.5:
            return 0.5 + (match_ratio - 0.5) * 0.8  # 0.5-0.9
        else:
            return match_ratio  # 0-0.5
    
    def _extract_query_entities(self, query: str) -> Dict:
        """
        ä»Žç”¨æˆ·æŸ¥è¯¢ä¸­æå–å®žä½“ï¼ˆç”¨äºŽç²¾å‡†åŒ¹é…ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            Dict: æå–çš„å®žä½“
        """
        # ç®€å•çš„å…³é”®è¯æå–
        # æ³¨æ„ï¼šä¸»è¦çš„ç»“æž„åŒ–æå–ç”± LLM å®Œæˆï¼ˆåœ¨ moment_manager çš„ _extract_structured_infoï¼‰
        # è¿™é‡Œåªæ˜¯è¾…åŠ©çš„ç®€å•è§„åˆ™åŒ¹é…
        entities = {
            "people": [],
            "places": [],
            "time_markers": [],
            "numbers": [],
            "events": []
        }
        
        import re
        
        # äººåæå–ï¼šä¸ç”¨å¤æ‚æ­£åˆ™ï¼Œç›´æŽ¥ä»ŽåŽ†å² Moments ä¸­å·²çŸ¥çš„äººååŒ¹é…
        # ï¼ˆå› ä¸ºç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰ä»£ç§°ã€è‹±æ–‡åç­‰ï¼Œæ­£åˆ™æ— æ³•è¦†ç›–æ‰€æœ‰æƒ…å†µï¼‰
        # è¿™ä¸ªåŠŸèƒ½ä¸»è¦ä¾èµ– LLM çš„ç»“æž„åŒ–æå–
        
        # æå–åœ°ç‚¹å…³é”®è¯
        place_keywords = ['è·¯', 'è¡—', 'åŒº', 'å¸‚', 'åº—', 'çº¿', 'å·çº¿', 'åœ°é“', 'å…¬å¸', 'å’–å•¡', 'Coffee']
        for word in query.split():
            if any(kw in word for kw in place_keywords):
                entities["places"].append(word)
        
        # æå–æ—¶é—´æ ‡è®°
        time_keywords = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥', 'æ˜¨å¤©', 'ä»Šå¤©', 'æ˜Žå¤©', 'ä¸Šæ¬¡', 'ä¹‹å‰']
        for kw in time_keywords:
            if kw in query:
                entities["time_markers"].append(kw)
        
        # æå–æ•°å­—
        numbers = re.findall(r'\d+', query)
        entities["numbers"] = numbers
        
        # æå–äº‹ä»¶å…³é”®è¯
        event_keywords = ['è¿Ÿåˆ°', 'è€ƒè¯•', 'å‡èŒ', 'å‚¬å©š', 'åƒé¥­', 'å¤±æŽ§', 'å·¥ä½œ', 'è·³æ§½']
        for kw in event_keywords:
            if kw in query:
                entities["events"].append(kw)
        
        return entities
    
    def _calculate_entity_match_score(self, moment: Dict, query_entities: Dict, query: str) -> float:
        """
        è®¡ç®— Moment ä¸ŽæŸ¥è¯¢å®žä½“çš„åŒ¹é…åˆ†æ•°
        
        Args:
            moment: Moment æ•°æ®
            query_entities: æŸ¥è¯¢ä¸­æå–çš„å®žä½“
            query: åŽŸå§‹æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            float: åŒ¹é…åˆ†æ•°ï¼ˆ0-100ï¼‰
        """
        score = 0.0
        
        # èŽ·å– Moment çš„ç»“æž„åŒ–ä¿¡æ¯
        moment_entities = moment.get('entities', {})
        
        # å¦‚æžœ Moment æ²¡æœ‰ entitiesï¼ˆæ—§æ•°æ®ï¼‰ï¼Œå›žé€€åˆ°æ–‡æœ¬åŒ¹é…
        if not moment_entities:
            # å›žé€€ï¼šç®€å•çš„å…³é”®è¯åŒ¹é…
            conversation = ""
            for msg in moment.get('messages', []):
                conversation += msg['content'] + " "
            
            # è®¡ç®—æŸ¥è¯¢å…³é”®è¯åœ¨å¯¹è¯ä¸­çš„å‡ºçŽ°æ¬¡æ•°
            keywords = self._extract_keywords(query)
            for kw in keywords:
                if kw in conversation:
                    score += 1
            
            return score
        
        # æƒé‡é…ç½®
        weights = {
            "people": 10,      # äººååŒ¹é…æƒé‡æœ€é«˜
            "places": 8,       # åœ°ç‚¹åŒ¹é…æ¬¡é«˜
            "numbers": 5,      # æ•°å­—åŒ¹é…
            "time_markers": 3, # æ—¶é—´æ ‡è®°
            "events": 2        # äº‹ä»¶å…³é”®è¯
        }
        
        # è®¡ç®—å„ç±»å®žä½“çš„åŒ¹é…åˆ†æ•°
        for entity_type, weight in weights.items():
            query_items = query_entities.get(entity_type, [])
            moment_items = moment_entities.get(entity_type, [])
            
            # è®¡ç®—äº¤é›†
            matched = set(query_items) & set(moment_items)
            score += len(matched) * weight
        
        # è¿‘æœŸåŠ æƒï¼ˆæœ€è¿‘çš„ Moment ç•¥å¾®åŠ åˆ†ï¼‰
        try:
            timestamp = datetime.fromisoformat(moment.get('timestamp', ''))
            age_days = (datetime.now() - timestamp).days
            
            # 7å¤©å†…çš„ Momentï¼Œæ¯å¤©åŠ  0.5 åˆ†
            if age_days < 7:
                score += (7 - age_days) * 0.5
        except:
            pass
        
        return score


# ============================================================
# æµ‹è¯•ä»£ç 
# ============================================================

def test_context_rag():
    """æµ‹è¯• Context RAG"""
    
    print("\n" + "="*60)
    print("ðŸ§ª æµ‹è¯• Context RAG")
    print("="*60 + "\n")
    
    rag = ContextRAG()
    
    # æµ‹è¯•1: åˆ›å»ºä¸€äº›æµ‹è¯• Moments
    print("ðŸ“ åˆ›å»ºæµ‹è¯• Moments...")
    from moment_manager import MomentManager
    
    manager = MomentManager()
    
    # Moment 1: å…³äºŽ project
    manager.start_new_moment()
    manager.add_message("user", "æˆ‘æœ‰ä¸ªå¾ˆéš¾çš„ projectï¼Œä¸çŸ¥é“èƒ½ä¸èƒ½åšæˆ", "worry")
    manager.add_message("assistant", "å¬èµ·æ¥æ˜¯ä¸ªå¾ˆæœ‰æŒ‘æˆ˜çš„é¡¹ç›®", "neutral")
    manager.end_moment()
    
    # Moment 2: å…³äºŽå·¥ä½œ
    manager.start_new_moment()
    manager.add_message("user", "ä»Šå¤©å·¥ä½œå¥½ç´¯", "sadness")
    manager.add_message("assistant", "è¾›è‹¦äº†", "neutral")
    manager.end_moment()
    
    print("âœ… æµ‹è¯• Moments åˆ›å»ºå®Œæˆ\n")
    
    # æµ‹è¯•2: å…³é”®è¯æ£€ç´¢
    print("ðŸ“ æµ‹è¯•å…³é”®è¯æ£€ç´¢...")
    results = rag.search_by_keywords(["project", "éš¾"], top_k=2)
    print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ Moments")
    
    # æµ‹è¯•3: å†…å®¹æ£€ç´¢
    print("\nðŸ“ æµ‹è¯•å†…å®¹æ£€ç´¢...")
    query = "æˆ‘åšæˆäº†ä¹‹å‰é‚£ä¸ª project"
    results = rag.search_by_content(query, top_k=1)
    print(f"   æŸ¥è¯¢: {query}")
    print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ Moments")
    
    # æµ‹è¯•4: ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º
    print("\nðŸ“ æµ‹è¯•ä¸Šä¸‹æ–‡æç¤ºç”Ÿæˆ...")
    context = rag.generate_context_prompt(query, max_context=1)
    print(f"   ä¸Šä¸‹æ–‡æç¤º:\n{context}")
    
    # æµ‹è¯•5: èŽ·å–æœ€è¿‘ Moments
    print("\nðŸ“ æµ‹è¯•èŽ·å–æœ€è¿‘ Moments...")
    recent = rag.get_recent_moments(n=3)
    print(f"   æœ€è¿‘ {len(recent)} ä¸ª Moments")
    
    print("\n" + "="*60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*60 + "\n")


if __name__ == "__main__":
    test_context_rag()